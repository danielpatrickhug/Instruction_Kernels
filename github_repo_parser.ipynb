{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17Ke5VkBRFkM1RIX9Vi1LDxLTGZGGN_vS",
      "authorship_tag": "ABX9TyMNEkFXlSuGq4S9TXbXIKmx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpatrickhug/Sentence_Kernels/blob/main/github_repo_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements and updating python version to 3.10\n"
      ],
      "metadata": {
        "id": "pyDPPnH5yc88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "pip install openai\n",
        "pip install sentence-transformers\n",
        "pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D88dtMQppKN1",
        "outputId": "a86ffae4-3c5e-4332-9a6d-e18a1394e89f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-7' coro=<ScriptMagics.shebang.<locals>._handle_stream() done, defined at /usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:211> exception=ValueError('Separator is not found, and chunk exceed the limit')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/streams.py\", line 524, in readline\n",
            "    line = await self.readuntil(sep)\n",
            "  File \"/usr/lib/python3.10/asyncio/streams.py\", line 602, in readuntil\n",
            "    raise exceptions.LimitOverrunError(\n",
            "asyncio.exceptions.LimitOverrunError: Separator is not found, and chunk exceed the limit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\", line 213, in _handle_stream\n",
            "    line = (await stream.readline()).decode(\"utf8\", errors=\"replace\")\n",
            "  File \"/usr/lib/python3.10/asyncio/streams.py\", line 533, in readline\n",
            "    raise ValueError(e.args[0])\n",
            "ValueError: Separator is not found, and chunk exceed the limit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make sure to restart runtime"
      ],
      "metadata": {
        "id": "OaP87gz-ccIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "#install python 3.9 and dev utils\n",
        "#you may not need all the dev libraries, but I haven't tested which aren't necessary.\n",
        "sudo apt-get update -y\n",
        "sudo apt-get install python3.10 python3.10-dev python3.10-distutils libpython3.10-dev\n",
        "\n",
        "#change alternatives\n",
        "sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2\n",
        "\n",
        "#Check that it points at the right location\n",
        "python3 --version\n",
        "\n",
        "# install pip\n",
        "curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "python3 get-pip.py --force-reinstall\n",
        "\n",
        "#install colab's dependencies\n",
        "python3 -m pip install ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "# link to the old google package\n",
        "ln -s /usr/local/lib/python3.8/dist-packages/google \\\n",
        "       /usr/local/lib/python3.10/dist-packages/google\n",
        "\n",
        "# There has got to be a better way to do this...but there's a bad import in some of the colab files\n",
        "# IPython no longer exposes traitlets like this, it's a separate package now\n",
        "sed -i \"s/from IPython.utils import traitlets as _traitlets/import traitlets as _traitlets/\" /usr/local/lib/python3.10/dist-packages/google/colab/*.py\n",
        "sed -i \"s/from IPython.utils import traitlets/import traitlets/\" /usr/local/lib/python3.10/dist-packages/google/colab/*.py"
      ],
      "metadata": {
        "id": "ZuX-HHYJy_t7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eoMMiuuyj42",
        "outputId": "a93f380c-bae6-43f3-b40f-f4ffb23bfffc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "tKQGmkGB5e4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import astor\n",
        "from getpass import getpass\n",
        "import openai\n",
        "import os\n",
        "import inspect\n",
        "import json\n"
      ],
      "metadata": {
        "id": "pYUvw4APoSFL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI key"
      ],
      "metadata": {
        "id": "L8Wa1YL-5gfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_secret = getpass('Enter the secret value: ')\n",
        "# Set up OpenAI API credentials\n",
        "openai.api_key = openai_secret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUWDfkEmo3Ig",
        "outputId": "5555a6f4-acce-42b9-ecc9-ae7245522aa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Repo of interest"
      ],
      "metadata": {
        "id": "iSeKDGzK5j7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "git clone https://github.com/MaartenGr/BERTopic.git"
      ],
      "metadata": {
        "id": "iqn84DMuwfmL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "git clone https://github.com/LAION-AI/Open-Assistant.git"
      ],
      "metadata": {
        "id": "iZ62990GpUgr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse Repo Dir and extract Imports, Globals, Classes(methods), Functions"
      ],
      "metadata": {
        "id": "D76BYM2T5pQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "\n",
        "def parse_python_file(file_path):\n",
        "    parsed_contents = {\"imports\": [], \"globals\": [], \"classes\": [], \"functions\": [],}\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        file_contents = file.read()\n",
        "        parsed_tree = ast.parse(file_contents)\n",
        "\n",
        "    for node in ast.iter_child_nodes(parsed_tree):\n",
        "        if isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):\n",
        "            parsed_contents[\"imports\"].append(astor.to_source(node).strip())\n",
        "        elif isinstance(node, ast.Assign) and len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n",
        "            parsed_contents[\"globals\"].append(astor.to_source(node).strip())\n",
        "        elif isinstance(node, ast.FunctionDef):\n",
        "            if node.name == \"main\":\n",
        "                parsed_contents[\"functions\"].append(ast.get_source_segment(file_contents, node))\n",
        "            else:\n",
        "                parsed_contents[\"functions\"].append(ast.get_source_segment(file_contents, node))\n",
        "        elif isinstance(node, ast.ClassDef):\n",
        "            parsed_contents[\"classes\"].append(ast.get_source_segment(file_contents, node))\n",
        "\n",
        "    return parsed_contents\n",
        "\n",
        "def get_methods(class_or_str):\n",
        "    if isinstance(class_or_str, str):\n",
        "        class_or_str = ast.parse(class_or_str)\n",
        "\n",
        "    method_nodes = [node for node in ast.iter_child_nodes(class_or_str) if isinstance(node, ast.FunctionDef)]\n",
        "    method_sources = []\n",
        "    for node in method_nodes:\n",
        "        source_lines, _ = ast.get_source_segment(class_or_str, node)\n",
        "        method_sources.append(''.join(source_lines).strip())\n",
        "    return method_sources\n",
        "\n",
        "def parse_github_repo(local_dir):\n",
        "    parsed_files = []\n",
        "    content_labels = {0: \"imports\", 1:\"globals\", 2: \"classes\", 3: \"functions\", 4: \"main\", 5: \"file_name\"}\n",
        "\n",
        "    for root, dirs, files in os.walk(local_dir):\n",
        "        for file_name in files:\n",
        "            if file_name.endswith(\".py\"):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                parsed_contents = parse_python_file(file_path)\n",
        "                content = {content_labels[i]: v for i, v in enumerate(parsed_contents.values())}\n",
        "                content[content_labels[5]] = file_path\n",
        "                parsed_files.append(content)\n",
        "\n",
        "    return parsed_files\n"
      ],
      "metadata": {
        "id": "7fJzcn1h0-gy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assistant Tasks"
      ],
      "metadata": {
        "id": "-X3Uu9C4LO_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_system_prompts(git_repo_path, file_name):\n",
        "    system_prompts = {\n",
        "        \"summary\": f\"\"\"\n",
        "                    Summarize the code the GitHub repository: {git_repo_path} you're currently in the file {file_name}\n",
        "                    ChatGPT will use its advanced natural language processing capabilities to analyze the code and generate a concise summary that captures\n",
        "                    the main functionality and purpose of the codebase. Additionally, ChatGPT can provide insights into the programming languages and libraries used in the repository,\n",
        "                    as well as any notable features or functionalities that are present. Simply provide the necessary information, and let ChatGPT do the rest! \n",
        "                    \"\"\",\n",
        "        \"bug_finder\": f\"\"\"\n",
        "                    Help identify bugs in the codebase of the GitHub repository: {git_repo_path} you're currently in the file {file_name}.\n",
        "                    ChatGPT will analyze the codebase to identify potential bugs and provide suggestions on how to fix them.\n",
        "                    Let ChatGPT assist you in your debugging efforts and make your code more robust and reliable.\n",
        "                    \"\"\",\n",
        "        \"todo_labeler\": f\"\"\"\n",
        "                    Automatically label and generate TODO comments in the codebase of the GitHub repository: {git_repo_path} you're currently in the file {file_name}.\n",
        "                    ChatGPT will scan the codebase for any potential TODO tasks and categorize them based on their priority and complexity.\n",
        "                    Let ChatGPT help you stay organized and on top of your development tasks.\n",
        "                    \"\"\",\n",
        "        \"code_suggestions\": f\"\"\"\n",
        "                    Get suggestions for improving the codebase of the GitHub repository: {git_repo_path} you're currently in the file {file_name}.\n",
        "                    ChatGPT will analyze the codebase and provide suggestions for improving the code quality, optimizing performance, and enhancing functionality.\n",
        "                    Let ChatGPT help you take your codebase to the next level.\n",
        "                    \"\"\",\n",
        "        \"question_asking\": f\"\"\"\n",
        "                    Ask ChatGPT questions about the codebase of the GitHub repository: {git_repo_path} you're currently in the file {file_name}.\n",
        "                    ChatGPT asks questions that a new developer may ask about the codebase used in the repository, \n",
        "                    as well as answer the question with step by step reasoning as a senoir dev would. All responses should first ask a question and then answer with reasoning.\n",
        "                    \"\"\",\n",
        "        \"complement_code\": f\"\"\"\n",
        "                    Get compliments on the codebase of the GitHub repository: {git_repo_path} you're currently in the file {file_name}.\n",
        "                    ChatGPT will analyze the codebase and provide positive feedback on the structure, organization, readability, and other aspects of the code.\n",
        "                    Let ChatGPT help you feel good about your code and celebrate your accomplishments!\n",
        "                    \"\"\",\n",
        "        \"translate_python_to_rust\": f\"\"\"\n",
        "                    Translate Python code in the GitHub repository: {git_repo_path} you're currently in the file {file_name} to Rust.\n",
        "                    ChatGPT will use its natural language processing capabilities to translate Python code snippets to Rust code.\n",
        "                    Let ChatGPT help you understand Python code and communicate more effectively with your team in Rust.\n",
        "                    \"\"\"\n",
        "    }\n",
        "    return system_prompts\n"
      ],
      "metadata": {
        "id": "-UIOkNLpF5Rz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "nK5a3vi3LUXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chat_gpt_inference(messages: list):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def create_prompt_message_template(text, role=\"user\"):\n",
        "    if role not in [\"user\", \"assistant\"]:\n",
        "        raise ValueError(\"Not a valid role. Please use 'user' or 'assistant'.\")\n",
        "    return {\"role\": role, \"content\": text}\n",
        "\n",
        "def compose_inference(text_block, messages):\n",
        "    user_template = create_prompt_message_template(text_block, role=\"user\")\n",
        "    messages.append(user_template)\n",
        "    chat_resp = chat_gpt_inference(messages)\n",
        "    reply_text = chat_resp['choices'][0]['message']['content']\n",
        "    assistant_template = create_prompt_message_template(reply_text, role=\"assistant\")\n",
        "    messages.append(assistant_template)\n",
        "    return messages, reply_text\n",
        "    \n",
        "def process_transcript(segments, file_name, git_repo_path, output_file_path, system_prompt, task, code_type):\n",
        "    buffer = []\n",
        "    \n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    running_size = 0\n",
        "    with open(output_file_path, 'a') as f:\n",
        "        for i, sent in enumerate(segments):\n",
        "            text_block = f\"\"\"```{sent}```\"\"\"\n",
        "            \n",
        "            messages, reply_text = compose_inference(text_block, messages)\n",
        "            #print(reply_text)\n",
        "            row = {\"git_repo_path\": git_repo_path, \"file_name\": file_name, \"code_type\": code_type, \"system_task\": task, \"system_prompt\": system_prompt, \"conversation_history\": messages, \"assistant_reply\": reply_text}\n",
        "            json.dump(row, f)\n",
        "            f.write(\"\\n\")\n",
        "            \n",
        "    return messages"
      ],
      "metadata": {
        "id": "J6C8FqZpoe7B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git_repo_path = \"/content/Open-Assistant/backend/oasst_backend/utils\"\n",
        "contents = parse_github_repo(git_repo_path)\n",
        "print(len(contents))\n",
        "\n",
        "for cont in contents:\n",
        "    system_prompts = format_system_prompts(git_repo_path, cont['file_name'])\n",
        "    for k, v in zip(system_prompts.keys(), system_prompts.values()):\n",
        "        func_task = k\n",
        "        out_file_name = f\"open_assistant_utils_{func_task}\"\n",
        "        print(f\"file_name: {cont['file_name']}\")\n",
        "        num_funcs = len(cont['functions'])\n",
        "        num_classes = len(cont['classes'])\n",
        "        print(f\"Imports: {cont['imports']}\")\n",
        "        #messages = process_transcript(cont['imports'], cont['file_name'], git_repo_path, f\"{out_file_name}.jsonl\", system_prompts[import_task], import_task, \"imports\")\n",
        "        if num_funcs > 0 or num_classes > 0:\n",
        "            print(f\"functions: {cont['functions']}\")\n",
        "                \n",
        "            messages = process_transcript(cont['functions'], cont['file_name'], git_repo_path, f\"/content/drive/MyDrive/open_assistant_utils_auto_mod/{out_file_name}.jsonl\", system_prompts[func_task], func_task, \"functions\")\n",
        "            print(f\"Classes: {cont['classes']}\")\n",
        "            for cls in cont['classes']:\n",
        "                cls_funcs = get_methods(cls)\n",
        "\n",
        "                print(f\"len of class: {len(cls)}\")\n",
        "                for method in cls_funcs:\n",
        "                    print(f\"len of method: {len(method)}\")\n",
        "            print(num_funcs, num_classes)\n",
        "            print('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKRpatkJpg9g",
        "outputId": "12387f22-c78f-4696-9b53-f95a7d308ba0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/ranking.py\n",
            "Imports: ['from typing import List', 'import numpy as np']\n",
            "functions: ['def head_to_head_votes(ranks: List[List[int]]):\\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\\n    names = sorted(ranks[0])\\n    ranks = np.array(ranks)\\n    # we want the sorted indices\\n    ranks = np.argsort(ranks, axis=1)\\n    for i in range(ranks.shape[1]):\\n        for j in range(i + 1, ranks.shape[1]):\\n            # now count the cases someone voted for i over j\\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\\n            tallies[i, j] = over_j\\n            # tallies[i,j] = over_i\\n            tallies[j, i] = over_i\\n            # tallies[j,i] = over_j\\n    return tallies, names', 'def cycle_detect(pairs):\\n    \"\"\"Recursively detect cylces by removing condorcet losers until either only one pair is left or condorcet loosers no longer exist\\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\\n\\n\\n    Returns\\n    -------\\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\\n\\n\\n    \"\"\"\\n    # get all condorcet losers (pairs that loose to all other pairs)\\n    # idea: filter all losers that are never winners\\n    # print(\"pairs\", pairs)\\n    if len(pairs) <= 1:\\n        return False\\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\\n    if len(losers) == 0:\\n        # if we recursively removed pairs, and at some point we did not have\\n        # a condorcet loser, that means everything is both a winner and loser,\\n        # yielding at least one (winner,loser), (loser,winner) pair\\n        return True\\n\\n    new = []\\n    for p in pairs:\\n        if p[1] not in losers:\\n            new.append(p)\\n    return cycle_detect(np.array(new))', 'def get_winner(pairs):\\n    \"\"\"\\n    This returns _one_ concordant winner.\\n    It could be that there are multiple concordant winners, but in our case\\n    since we are interested in a ranking, we have to choose one at random.\\n    \"\"\"\\n    losers = np.unique(pairs[:, 1]).astype(int)\\n    winners = np.unique(pairs[:, 0]).astype(int)\\n    for w in winners:\\n        if w not in losers:\\n            return w', 'def get_ranking(pairs):\\n    \"\"\"\\n    Abuses concordance property to get a (not necessarily unique) ranking.\\n    The lack of uniqueness is due to the potential existence of multiple\\n    equally ranked winners. We have to pick one, which is where\\n    the non-uniqueness comes from\\n    \"\"\"\\n    if len(pairs) == 1:\\n        return list(pairs[0])\\n    w = get_winner(pairs)\\n    # now remove the winner from the list of pairs\\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\\n    return [w] + get_ranking(p_new)', 'def ranked_pairs(ranks: List[List[int]]):\\n    \"\"\"\\n    Expects a list of rankings for an item like:\\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\\n    This code is quite brain melting, but the idea is the following:\\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\\n    2. take all combinations that win more than they loose and sort those by how often they win\\n    3. use that to create an (implicit) directed graph\\n    4. recursively extract nodes from the graph that do not have incoming edges\\n    5. said recursive list is the ranking\\n    \"\"\"\\n    tallies, names = head_to_head_votes(ranks)\\n    tallies = tallies - tallies.T\\n    # note: the resulting tally matrix should be skew-symmetric\\n    # order by strength of victory (using tideman\\'s original method, don\\'t think it would make a difference for us)\\n    sorted_majorities = []\\n    for i in range(len(ranks[0])):\\n        for j in range(len(ranks[0])):\\n            # you can never prefer yourself over yourself\\n            # we also have to pick one of the two choices,\\n            # if the preference is exactly zero...\\n            if tallies[i, j] >= 0 and i != j:\\n                sorted_majorities.append((i, j, tallies[i, j]))\\n    # we don\\'t explicitly deal with tied majorities here\\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\\n    # now do lock ins\\n    lock_ins = []\\n    for x, y, _ in sorted_majorities:\\n        # invariant: lock_ins has no cycles here\\n        lock_ins.append((x, y))\\n        # print(\"lock ins are now\",np.array(lock_ins))\\n        if cycle_detect(np.array(lock_ins)):\\n            # print(\"backup: cycle detected\")\\n            # if there\\'s a cycle, delete the new addition and continue\\n            lock_ins = lock_ins[:-1]\\n    # now simply return all winners in order, and attach the losers\\n    # to the back. This is because the overall loser might not be unique\\n    # and (by concordance property) may never exist in any winning set to begin with.\\n    # (otherwise he would either not be the loser, or cycles exist!)\\n    # Since there could be multiple overall losers, we just return them in any order\\n    # as we are unable to find a closer ranking\\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\\n    conversion = [names[n] for n in numerical_ranks]\\n    return conversion']\n",
            "Classes: []\n",
            "5 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/ranking.py\n",
            "Imports: ['from typing import List', 'import numpy as np']\n",
            "functions: ['def head_to_head_votes(ranks: List[List[int]]):\\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\\n    names = sorted(ranks[0])\\n    ranks = np.array(ranks)\\n    # we want the sorted indices\\n    ranks = np.argsort(ranks, axis=1)\\n    for i in range(ranks.shape[1]):\\n        for j in range(i + 1, ranks.shape[1]):\\n            # now count the cases someone voted for i over j\\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\\n            tallies[i, j] = over_j\\n            # tallies[i,j] = over_i\\n            tallies[j, i] = over_i\\n            # tallies[j,i] = over_j\\n    return tallies, names', 'def cycle_detect(pairs):\\n    \"\"\"Recursively detect cylces by removing condorcet losers until either only one pair is left or condorcet loosers no longer exist\\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\\n\\n\\n    Returns\\n    -------\\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\\n\\n\\n    \"\"\"\\n    # get all condorcet losers (pairs that loose to all other pairs)\\n    # idea: filter all losers that are never winners\\n    # print(\"pairs\", pairs)\\n    if len(pairs) <= 1:\\n        return False\\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\\n    if len(losers) == 0:\\n        # if we recursively removed pairs, and at some point we did not have\\n        # a condorcet loser, that means everything is both a winner and loser,\\n        # yielding at least one (winner,loser), (loser,winner) pair\\n        return True\\n\\n    new = []\\n    for p in pairs:\\n        if p[1] not in losers:\\n            new.append(p)\\n    return cycle_detect(np.array(new))', 'def get_winner(pairs):\\n    \"\"\"\\n    This returns _one_ concordant winner.\\n    It could be that there are multiple concordant winners, but in our case\\n    since we are interested in a ranking, we have to choose one at random.\\n    \"\"\"\\n    losers = np.unique(pairs[:, 1]).astype(int)\\n    winners = np.unique(pairs[:, 0]).astype(int)\\n    for w in winners:\\n        if w not in losers:\\n            return w', 'def get_ranking(pairs):\\n    \"\"\"\\n    Abuses concordance property to get a (not necessarily unique) ranking.\\n    The lack of uniqueness is due to the potential existence of multiple\\n    equally ranked winners. We have to pick one, which is where\\n    the non-uniqueness comes from\\n    \"\"\"\\n    if len(pairs) == 1:\\n        return list(pairs[0])\\n    w = get_winner(pairs)\\n    # now remove the winner from the list of pairs\\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\\n    return [w] + get_ranking(p_new)', 'def ranked_pairs(ranks: List[List[int]]):\\n    \"\"\"\\n    Expects a list of rankings for an item like:\\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\\n    This code is quite brain melting, but the idea is the following:\\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\\n    2. take all combinations that win more than they loose and sort those by how often they win\\n    3. use that to create an (implicit) directed graph\\n    4. recursively extract nodes from the graph that do not have incoming edges\\n    5. said recursive list is the ranking\\n    \"\"\"\\n    tallies, names = head_to_head_votes(ranks)\\n    tallies = tallies - tallies.T\\n    # note: the resulting tally matrix should be skew-symmetric\\n    # order by strength of victory (using tideman\\'s original method, don\\'t think it would make a difference for us)\\n    sorted_majorities = []\\n    for i in range(len(ranks[0])):\\n        for j in range(len(ranks[0])):\\n            # you can never prefer yourself over yourself\\n            # we also have to pick one of the two choices,\\n            # if the preference is exactly zero...\\n            if tallies[i, j] >= 0 and i != j:\\n                sorted_majorities.append((i, j, tallies[i, j]))\\n    # we don\\'t explicitly deal with tied majorities here\\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\\n    # now do lock ins\\n    lock_ins = []\\n    for x, y, _ in sorted_majorities:\\n        # invariant: lock_ins has no cycles here\\n        lock_ins.append((x, y))\\n        # print(\"lock ins are now\",np.array(lock_ins))\\n        if cycle_detect(np.array(lock_ins)):\\n            # print(\"backup: cycle detected\")\\n            # if there\\'s a cycle, delete the new addition and continue\\n            lock_ins = lock_ins[:-1]\\n    # now simply return all winners in order, and attach the losers\\n    # to the back. This is because the overall loser might not be unique\\n    # and (by concordance property) may never exist in any winning set to begin with.\\n    # (otherwise he would either not be the loser, or cycles exist!)\\n    # Since there could be multiple overall losers, we just return them in any order\\n    # as we are unable to find a closer ranking\\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\\n    conversion = [names[n] for n in numerical_ranks]\\n    return conversion']\n",
            "Classes: []\n",
            "5 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/ranking.py\n",
            "Imports: ['from typing import List', 'import numpy as np']\n",
            "functions: ['def head_to_head_votes(ranks: List[List[int]]):\\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\\n    names = sorted(ranks[0])\\n    ranks = np.array(ranks)\\n    # we want the sorted indices\\n    ranks = np.argsort(ranks, axis=1)\\n    for i in range(ranks.shape[1]):\\n        for j in range(i + 1, ranks.shape[1]):\\n            # now count the cases someone voted for i over j\\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\\n            tallies[i, j] = over_j\\n            # tallies[i,j] = over_i\\n            tallies[j, i] = over_i\\n            # tallies[j,i] = over_j\\n    return tallies, names', 'def cycle_detect(pairs):\\n    \"\"\"Recursively detect cylces by removing condorcet losers until either only one pair is left or condorcet loosers no longer exist\\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\\n\\n\\n    Returns\\n    -------\\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\\n\\n\\n    \"\"\"\\n    # get all condorcet losers (pairs that loose to all other pairs)\\n    # idea: filter all losers that are never winners\\n    # print(\"pairs\", pairs)\\n    if len(pairs) <= 1:\\n        return False\\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\\n    if len(losers) == 0:\\n        # if we recursively removed pairs, and at some point we did not have\\n        # a condorcet loser, that means everything is both a winner and loser,\\n        # yielding at least one (winner,loser), (loser,winner) pair\\n        return True\\n\\n    new = []\\n    for p in pairs:\\n        if p[1] not in losers:\\n            new.append(p)\\n    return cycle_detect(np.array(new))', 'def get_winner(pairs):\\n    \"\"\"\\n    This returns _one_ concordant winner.\\n    It could be that there are multiple concordant winners, but in our case\\n    since we are interested in a ranking, we have to choose one at random.\\n    \"\"\"\\n    losers = np.unique(pairs[:, 1]).astype(int)\\n    winners = np.unique(pairs[:, 0]).astype(int)\\n    for w in winners:\\n        if w not in losers:\\n            return w', 'def get_ranking(pairs):\\n    \"\"\"\\n    Abuses concordance property to get a (not necessarily unique) ranking.\\n    The lack of uniqueness is due to the potential existence of multiple\\n    equally ranked winners. We have to pick one, which is where\\n    the non-uniqueness comes from\\n    \"\"\"\\n    if len(pairs) == 1:\\n        return list(pairs[0])\\n    w = get_winner(pairs)\\n    # now remove the winner from the list of pairs\\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\\n    return [w] + get_ranking(p_new)', 'def ranked_pairs(ranks: List[List[int]]):\\n    \"\"\"\\n    Expects a list of rankings for an item like:\\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\\n    This code is quite brain melting, but the idea is the following:\\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\\n    2. take all combinations that win more than they loose and sort those by how often they win\\n    3. use that to create an (implicit) directed graph\\n    4. recursively extract nodes from the graph that do not have incoming edges\\n    5. said recursive list is the ranking\\n    \"\"\"\\n    tallies, names = head_to_head_votes(ranks)\\n    tallies = tallies - tallies.T\\n    # note: the resulting tally matrix should be skew-symmetric\\n    # order by strength of victory (using tideman\\'s original method, don\\'t think it would make a difference for us)\\n    sorted_majorities = []\\n    for i in range(len(ranks[0])):\\n        for j in range(len(ranks[0])):\\n            # you can never prefer yourself over yourself\\n            # we also have to pick one of the two choices,\\n            # if the preference is exactly zero...\\n            if tallies[i, j] >= 0 and i != j:\\n                sorted_majorities.append((i, j, tallies[i, j]))\\n    # we don\\'t explicitly deal with tied majorities here\\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\\n    # now do lock ins\\n    lock_ins = []\\n    for x, y, _ in sorted_majorities:\\n        # invariant: lock_ins has no cycles here\\n        lock_ins.append((x, y))\\n        # print(\"lock ins are now\",np.array(lock_ins))\\n        if cycle_detect(np.array(lock_ins)):\\n            # print(\"backup: cycle detected\")\\n            # if there\\'s a cycle, delete the new addition and continue\\n            lock_ins = lock_ins[:-1]\\n    # now simply return all winners in order, and attach the losers\\n    # to the back. This is because the overall loser might not be unique\\n    # and (by concordance property) may never exist in any winning set to begin with.\\n    # (otherwise he would either not be the loser, or cycles exist!)\\n    # Since there could be multiple overall losers, we just return them in any order\\n    # as we are unable to find a closer ranking\\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\\n    conversion = [names[n] for n in numerical_ranks]\\n    return conversion']\n",
            "Classes: []\n",
            "5 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/ranking.py\n",
            "Imports: ['from typing import List', 'import numpy as np']\n",
            "functions: ['def head_to_head_votes(ranks: List[List[int]]):\\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\\n    names = sorted(ranks[0])\\n    ranks = np.array(ranks)\\n    # we want the sorted indices\\n    ranks = np.argsort(ranks, axis=1)\\n    for i in range(ranks.shape[1]):\\n        for j in range(i + 1, ranks.shape[1]):\\n            # now count the cases someone voted for i over j\\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\\n            tallies[i, j] = over_j\\n            # tallies[i,j] = over_i\\n            tallies[j, i] = over_i\\n            # tallies[j,i] = over_j\\n    return tallies, names', 'def cycle_detect(pairs):\\n    \"\"\"Recursively detect cylces by removing condorcet losers until either only one pair is left or condorcet loosers no longer exist\\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\\n\\n\\n    Returns\\n    -------\\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\\n\\n\\n    \"\"\"\\n    # get all condorcet losers (pairs that loose to all other pairs)\\n    # idea: filter all losers that are never winners\\n    # print(\"pairs\", pairs)\\n    if len(pairs) <= 1:\\n        return False\\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\\n    if len(losers) == 0:\\n        # if we recursively removed pairs, and at some point we did not have\\n        # a condorcet loser, that means everything is both a winner and loser,\\n        # yielding at least one (winner,loser), (loser,winner) pair\\n        return True\\n\\n    new = []\\n    for p in pairs:\\n        if p[1] not in losers:\\n            new.append(p)\\n    return cycle_detect(np.array(new))', 'def get_winner(pairs):\\n    \"\"\"\\n    This returns _one_ concordant winner.\\n    It could be that there are multiple concordant winners, but in our case\\n    since we are interested in a ranking, we have to choose one at random.\\n    \"\"\"\\n    losers = np.unique(pairs[:, 1]).astype(int)\\n    winners = np.unique(pairs[:, 0]).astype(int)\\n    for w in winners:\\n        if w not in losers:\\n            return w', 'def get_ranking(pairs):\\n    \"\"\"\\n    Abuses concordance property to get a (not necessarily unique) ranking.\\n    The lack of uniqueness is due to the potential existence of multiple\\n    equally ranked winners. We have to pick one, which is where\\n    the non-uniqueness comes from\\n    \"\"\"\\n    if len(pairs) == 1:\\n        return list(pairs[0])\\n    w = get_winner(pairs)\\n    # now remove the winner from the list of pairs\\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\\n    return [w] + get_ranking(p_new)', 'def ranked_pairs(ranks: List[List[int]]):\\n    \"\"\"\\n    Expects a list of rankings for an item like:\\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\\n    This code is quite brain melting, but the idea is the following:\\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\\n    2. take all combinations that win more than they loose and sort those by how often they win\\n    3. use that to create an (implicit) directed graph\\n    4. recursively extract nodes from the graph that do not have incoming edges\\n    5. said recursive list is the ranking\\n    \"\"\"\\n    tallies, names = head_to_head_votes(ranks)\\n    tallies = tallies - tallies.T\\n    # note: the resulting tally matrix should be skew-symmetric\\n    # order by strength of victory (using tideman\\'s original method, don\\'t think it would make a difference for us)\\n    sorted_majorities = []\\n    for i in range(len(ranks[0])):\\n        for j in range(len(ranks[0])):\\n            # you can never prefer yourself over yourself\\n            # we also have to pick one of the two choices,\\n            # if the preference is exactly zero...\\n            if tallies[i, j] >= 0 and i != j:\\n                sorted_majorities.append((i, j, tallies[i, j]))\\n    # we don\\'t explicitly deal with tied majorities here\\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\\n    # now do lock ins\\n    lock_ins = []\\n    for x, y, _ in sorted_majorities:\\n        # invariant: lock_ins has no cycles here\\n        lock_ins.append((x, y))\\n        # print(\"lock ins are now\",np.array(lock_ins))\\n        if cycle_detect(np.array(lock_ins)):\\n            # print(\"backup: cycle detected\")\\n            # if there\\'s a cycle, delete the new addition and continue\\n            lock_ins = lock_ins[:-1]\\n    # now simply return all winners in order, and attach the losers\\n    # to the back. This is because the overall loser might not be unique\\n    # and (by concordance property) may never exist in any winning set to begin with.\\n    # (otherwise he would either not be the loser, or cycles exist!)\\n    # Since there could be multiple overall losers, we just return them in any order\\n    # as we are unable to find a closer ranking\\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\\n    conversion = [names[n] for n in numerical_ranks]\\n    return conversion']\n",
            "Classes: []\n",
            "5 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/ranking.py\n",
            "Imports: ['from typing import List', 'import numpy as np']\n",
            "functions: ['def head_to_head_votes(ranks: List[List[int]]):\\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\\n    names = sorted(ranks[0])\\n    ranks = np.array(ranks)\\n    # we want the sorted indices\\n    ranks = np.argsort(ranks, axis=1)\\n    for i in range(ranks.shape[1]):\\n        for j in range(i + 1, ranks.shape[1]):\\n            # now count the cases someone voted for i over j\\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\\n            tallies[i, j] = over_j\\n            # tallies[i,j] = over_i\\n            tallies[j, i] = over_i\\n            # tallies[j,i] = over_j\\n    return tallies, names', 'def cycle_detect(pairs):\\n    \"\"\"Recursively detect cylces by removing condorcet losers until either only one pair is left or condorcet loosers no longer exist\\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\\n\\n\\n    Returns\\n    -------\\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\\n\\n\\n    \"\"\"\\n    # get all condorcet losers (pairs that loose to all other pairs)\\n    # idea: filter all losers that are never winners\\n    # print(\"pairs\", pairs)\\n    if len(pairs) <= 1:\\n        return False\\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\\n    if len(losers) == 0:\\n        # if we recursively removed pairs, and at some point we did not have\\n        # a condorcet loser, that means everything is both a winner and loser,\\n        # yielding at least one (winner,loser), (loser,winner) pair\\n        return True\\n\\n    new = []\\n    for p in pairs:\\n        if p[1] not in losers:\\n            new.append(p)\\n    return cycle_detect(np.array(new))', 'def get_winner(pairs):\\n    \"\"\"\\n    This returns _one_ concordant winner.\\n    It could be that there are multiple concordant winners, but in our case\\n    since we are interested in a ranking, we have to choose one at random.\\n    \"\"\"\\n    losers = np.unique(pairs[:, 1]).astype(int)\\n    winners = np.unique(pairs[:, 0]).astype(int)\\n    for w in winners:\\n        if w not in losers:\\n            return w', 'def get_ranking(pairs):\\n    \"\"\"\\n    Abuses concordance property to get a (not necessarily unique) ranking.\\n    The lack of uniqueness is due to the potential existence of multiple\\n    equally ranked winners. We have to pick one, which is where\\n    the non-uniqueness comes from\\n    \"\"\"\\n    if len(pairs) == 1:\\n        return list(pairs[0])\\n    w = get_winner(pairs)\\n    # now remove the winner from the list of pairs\\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\\n    return [w] + get_ranking(p_new)', 'def ranked_pairs(ranks: List[List[int]]):\\n    \"\"\"\\n    Expects a list of rankings for an item like:\\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\\n    This code is quite brain melting, but the idea is the following:\\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\\n    2. take all combinations that win more than they loose and sort those by how often they win\\n    3. use that to create an (implicit) directed graph\\n    4. recursively extract nodes from the graph that do not have incoming edges\\n    5. said recursive list is the ranking\\n    \"\"\"\\n    tallies, names = head_to_head_votes(ranks)\\n    tallies = tallies - tallies.T\\n    # note: the resulting tally matrix should be skew-symmetric\\n    # order by strength of victory (using tideman\\'s original method, don\\'t think it would make a difference for us)\\n    sorted_majorities = []\\n    for i in range(len(ranks[0])):\\n        for j in range(len(ranks[0])):\\n            # you can never prefer yourself over yourself\\n            # we also have to pick one of the two choices,\\n            # if the preference is exactly zero...\\n            if tallies[i, j] >= 0 and i != j:\\n                sorted_majorities.append((i, j, tallies[i, j]))\\n    # we don\\'t explicitly deal with tied majorities here\\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\\n    # now do lock ins\\n    lock_ins = []\\n    for x, y, _ in sorted_majorities:\\n        # invariant: lock_ins has no cycles here\\n        lock_ins.append((x, y))\\n        # print(\"lock ins are now\",np.array(lock_ins))\\n        if cycle_detect(np.array(lock_ins)):\\n            # print(\"backup: cycle detected\")\\n            # if there\\'s a cycle, delete the new addition and continue\\n            lock_ins = lock_ins[:-1]\\n    # now simply return all winners in order, and attach the losers\\n    # to the back. This is because the overall loser might not be unique\\n    # and (by concordance property) may never exist in any winning set to begin with.\\n    # (otherwise he would either not be the loser, or cycles exist!)\\n    # Since there could be multiple overall losers, we just return them in any order\\n    # as we are unable to find a closer ranking\\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\\n    conversion = [names[n] for n in numerical_ranks]\\n    return conversion']\n",
            "Classes: []\n",
            "5 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/ranking.py\n",
            "Imports: ['from typing import List', 'import numpy as np']\n",
            "functions: ['def head_to_head_votes(ranks: List[List[int]]):\\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\\n    names = sorted(ranks[0])\\n    ranks = np.array(ranks)\\n    # we want the sorted indices\\n    ranks = np.argsort(ranks, axis=1)\\n    for i in range(ranks.shape[1]):\\n        for j in range(i + 1, ranks.shape[1]):\\n            # now count the cases someone voted for i over j\\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\\n            tallies[i, j] = over_j\\n            # tallies[i,j] = over_i\\n            tallies[j, i] = over_i\\n            # tallies[j,i] = over_j\\n    return tallies, names', 'def cycle_detect(pairs):\\n    \"\"\"Recursively detect cylces by removing condorcet losers until either only one pair is left or condorcet loosers no longer exist\\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\\n\\n\\n    Returns\\n    -------\\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\\n\\n\\n    \"\"\"\\n    # get all condorcet losers (pairs that loose to all other pairs)\\n    # idea: filter all losers that are never winners\\n    # print(\"pairs\", pairs)\\n    if len(pairs) <= 1:\\n        return False\\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\\n    if len(losers) == 0:\\n        # if we recursively removed pairs, and at some point we did not have\\n        # a condorcet loser, that means everything is both a winner and loser,\\n        # yielding at least one (winner,loser), (loser,winner) pair\\n        return True\\n\\n    new = []\\n    for p in pairs:\\n        if p[1] not in losers:\\n            new.append(p)\\n    return cycle_detect(np.array(new))', 'def get_winner(pairs):\\n    \"\"\"\\n    This returns _one_ concordant winner.\\n    It could be that there are multiple concordant winners, but in our case\\n    since we are interested in a ranking, we have to choose one at random.\\n    \"\"\"\\n    losers = np.unique(pairs[:, 1]).astype(int)\\n    winners = np.unique(pairs[:, 0]).astype(int)\\n    for w in winners:\\n        if w not in losers:\\n            return w', 'def get_ranking(pairs):\\n    \"\"\"\\n    Abuses concordance property to get a (not necessarily unique) ranking.\\n    The lack of uniqueness is due to the potential existence of multiple\\n    equally ranked winners. We have to pick one, which is where\\n    the non-uniqueness comes from\\n    \"\"\"\\n    if len(pairs) == 1:\\n        return list(pairs[0])\\n    w = get_winner(pairs)\\n    # now remove the winner from the list of pairs\\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\\n    return [w] + get_ranking(p_new)', 'def ranked_pairs(ranks: List[List[int]]):\\n    \"\"\"\\n    Expects a list of rankings for an item like:\\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\\n    This code is quite brain melting, but the idea is the following:\\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\\n    2. take all combinations that win more than they loose and sort those by how often they win\\n    3. use that to create an (implicit) directed graph\\n    4. recursively extract nodes from the graph that do not have incoming edges\\n    5. said recursive list is the ranking\\n    \"\"\"\\n    tallies, names = head_to_head_votes(ranks)\\n    tallies = tallies - tallies.T\\n    # note: the resulting tally matrix should be skew-symmetric\\n    # order by strength of victory (using tideman\\'s original method, don\\'t think it would make a difference for us)\\n    sorted_majorities = []\\n    for i in range(len(ranks[0])):\\n        for j in range(len(ranks[0])):\\n            # you can never prefer yourself over yourself\\n            # we also have to pick one of the two choices,\\n            # if the preference is exactly zero...\\n            if tallies[i, j] >= 0 and i != j:\\n                sorted_majorities.append((i, j, tallies[i, j]))\\n    # we don\\'t explicitly deal with tied majorities here\\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\\n    # now do lock ins\\n    lock_ins = []\\n    for x, y, _ in sorted_majorities:\\n        # invariant: lock_ins has no cycles here\\n        lock_ins.append((x, y))\\n        # print(\"lock ins are now\",np.array(lock_ins))\\n        if cycle_detect(np.array(lock_ins)):\\n            # print(\"backup: cycle detected\")\\n            # if there\\'s a cycle, delete the new addition and continue\\n            lock_ins = lock_ins[:-1]\\n    # now simply return all winners in order, and attach the losers\\n    # to the back. This is because the overall loser might not be unique\\n    # and (by concordance property) may never exist in any winning set to begin with.\\n    # (otherwise he would either not be the loser, or cycles exist!)\\n    # Since there could be multiple overall losers, we just return them in any order\\n    # as we are unable to find a closer ranking\\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\\n    conversion = [names[n] for n in numerical_ranks]\\n    return conversion']\n",
            "Classes: []\n",
            "5 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/__init__.py\n",
            "Imports: []\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/__init__.py\n",
            "Imports: []\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/__init__.py\n",
            "Imports: []\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/__init__.py\n",
            "Imports: []\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/__init__.py\n",
            "Imports: []\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/__init__.py\n",
            "Imports: []\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/database_utils.py\n",
            "Imports: ['from enum import IntEnum', 'from functools import wraps', 'from http import HTTPStatus', 'from typing import Callable', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_backend.database import engine', 'from oasst_shared.exceptions import OasstError, OasstErrorCode', 'from psycopg2.errors import DeadlockDetected, ExclusionViolation, SerializationFailure, UniqueViolation', 'from sqlalchemy.exc import OperationalError, PendingRollbackError', 'from sqlmodel import Session, SQLModel']\n",
            "functions: ['def managed_tx_method(auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT):\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_method(\\n    auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT\\n):\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = await f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = await f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def default_session_factory() -> Session:\\n    return Session(engine)', 'def managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = await f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = await f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator']\n",
            "Classes: ['class CommitMode(IntEnum):\\n    \"\"\"\\n    Commit modes for the managed tx methods\\n    \"\"\"\\n\\n    NONE = 0\\n    FLUSH = 1\\n    COMMIT = 2\\n    ROLLBACK = 3']\n",
            "len of class: 146\n",
            "5 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/database_utils.py\n",
            "Imports: ['from enum import IntEnum', 'from functools import wraps', 'from http import HTTPStatus', 'from typing import Callable', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_backend.database import engine', 'from oasst_shared.exceptions import OasstError, OasstErrorCode', 'from psycopg2.errors import DeadlockDetected, ExclusionViolation, SerializationFailure, UniqueViolation', 'from sqlalchemy.exc import OperationalError, PendingRollbackError', 'from sqlmodel import Session, SQLModel']\n",
            "functions: ['def managed_tx_method(auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT):\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_method(\\n    auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT\\n):\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = await f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = await f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def default_session_factory() -> Session:\\n    return Session(engine)', 'def managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = await f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = await f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator']\n",
            "Classes: ['class CommitMode(IntEnum):\\n    \"\"\"\\n    Commit modes for the managed tx methods\\n    \"\"\"\\n\\n    NONE = 0\\n    FLUSH = 1\\n    COMMIT = 2\\n    ROLLBACK = 3']\n",
            "len of class: 146\n",
            "5 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/database_utils.py\n",
            "Imports: ['from enum import IntEnum', 'from functools import wraps', 'from http import HTTPStatus', 'from typing import Callable', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_backend.database import engine', 'from oasst_shared.exceptions import OasstError, OasstErrorCode', 'from psycopg2.errors import DeadlockDetected, ExclusionViolation, SerializationFailure, UniqueViolation', 'from sqlalchemy.exc import OperationalError, PendingRollbackError', 'from sqlmodel import Session, SQLModel']\n",
            "functions: ['def managed_tx_method(auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT):\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_method(\\n    auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT\\n):\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = await f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = await f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def default_session_factory() -> Session:\\n    return Session(engine)', 'def managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = await f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = await f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator']\n",
            "Classes: ['class CommitMode(IntEnum):\\n    \"\"\"\\n    Commit modes for the managed tx methods\\n    \"\"\"\\n\\n    NONE = 0\\n    FLUSH = 1\\n    COMMIT = 2\\n    ROLLBACK = 3']\n",
            "len of class: 146\n",
            "5 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/database_utils.py\n",
            "Imports: ['from enum import IntEnum', 'from functools import wraps', 'from http import HTTPStatus', 'from typing import Callable', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_backend.database import engine', 'from oasst_shared.exceptions import OasstError, OasstErrorCode', 'from psycopg2.errors import DeadlockDetected, ExclusionViolation, SerializationFailure, UniqueViolation', 'from sqlalchemy.exc import OperationalError, PendingRollbackError', 'from sqlmodel import Session, SQLModel']\n",
            "functions: ['def managed_tx_method(auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT):\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_method(\\n    auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT\\n):\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = await f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = await f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def default_session_factory() -> Session:\\n    return Session(engine)', 'def managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = await f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = await f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator']\n",
            "Classes: ['class CommitMode(IntEnum):\\n    \"\"\"\\n    Commit modes for the managed tx methods\\n    \"\"\"\\n\\n    NONE = 0\\n    FLUSH = 1\\n    COMMIT = 2\\n    ROLLBACK = 3']\n",
            "len of class: 146\n",
            "5 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/database_utils.py\n",
            "Imports: ['from enum import IntEnum', 'from functools import wraps', 'from http import HTTPStatus', 'from typing import Callable', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_backend.database import engine', 'from oasst_shared.exceptions import OasstError, OasstErrorCode', 'from psycopg2.errors import DeadlockDetected, ExclusionViolation, SerializationFailure, UniqueViolation', 'from sqlalchemy.exc import OperationalError, PendingRollbackError', 'from sqlmodel import Session, SQLModel']\n",
            "functions: ['def managed_tx_method(auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT):\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_method(\\n    auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT\\n):\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = await f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = await f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def default_session_factory() -> Session:\\n    return Session(engine)', 'def managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = await f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = await f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator']\n",
            "Classes: ['class CommitMode(IntEnum):\\n    \"\"\"\\n    Commit modes for the managed tx methods\\n    \"\"\"\\n\\n    NONE = 0\\n    FLUSH = 1\\n    COMMIT = 2\\n    ROLLBACK = 3']\n",
            "len of class: 146\n",
            "5 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/database_utils.py\n",
            "Imports: ['from enum import IntEnum', 'from functools import wraps', 'from http import HTTPStatus', 'from typing import Callable', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_backend.database import engine', 'from oasst_shared.exceptions import OasstError, OasstErrorCode', 'from psycopg2.errors import DeadlockDetected, ExclusionViolation, SerializationFailure, UniqueViolation', 'from sqlalchemy.exc import OperationalError, PendingRollbackError', 'from sqlmodel import Session, SQLModel']\n",
            "functions: ['def managed_tx_method(auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT):\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_method(\\n    auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT\\n):\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(self, *args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        try:\\n                            result = await f(self, *args, **kwargs)\\n                            self.db.commit()\\n                            if isinstance(result, SQLModel):\\n                                self.db.refresh(result)\\n                            retry_exhausted = False\\n                            break\\n                        except PendingRollbackError as e:\\n                            logger.info(str(e))\\n                            self.db.rollback()\\n                        except OperationalError as e:\\n                            if e.orig is not None and isinstance(\\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\\n                            ):\\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                self.db.rollback()\\n                            else:\\n                                raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    result = await f(self, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        self.db.flush()\\n                        if isinstance(result, SQLModel):\\n                            self.db.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        self.db.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def default_session_factory() -> Session:\\n    return Session(engine)', 'def managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator', 'def async_managed_tx_function(\\n    auto_commit: CommitMode = CommitMode.COMMIT,\\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\\n    session_factory: Callable[..., Session] = default_session_factory,\\n):\\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\\n\\n    def decorator(f):\\n        @wraps(f)\\n        async def wrapped_f(*args, **kwargs):\\n            try:\\n                result = None\\n                if auto_commit == CommitMode.COMMIT:\\n                    retry_exhausted = True\\n                    for i in range(num_retries):\\n                        with session_factory() as session:\\n                            try:\\n                                result = await f(session, *args, **kwargs)\\n                                session.commit()\\n                                if isinstance(result, SQLModel):\\n                                    session.refresh(result)\\n                                retry_exhausted = False\\n                                break\\n                            except PendingRollbackError as e:\\n                                logger.info(str(e))\\n                                session.rollback()\\n                            except OperationalError as e:\\n                                if e.orig is not None and isinstance(\\n                                    e.orig,\\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\\n                                ):\\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\\n                                    session.rollback()\\n                                else:\\n                                    raise e\\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\\n                    if retry_exhausted:\\n                        raise OasstError(\\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\\n                        )\\n                else:\\n                    with session_factory() as session:\\n                        result = await f(session, *args, **kwargs)\\n                    if auto_commit == CommitMode.FLUSH:\\n                        session.flush()\\n                        if isinstance(result, SQLModel):\\n                            session.refresh(result)\\n                    elif auto_commit == CommitMode.ROLLBACK:\\n                        session.rollback()\\n                return result\\n            except Exception as e:\\n                logger.info(str(e))\\n                raise e\\n\\n        return wrapped_f\\n\\n    return decorator']\n",
            "Classes: ['class CommitMode(IntEnum):\\n    \"\"\"\\n    Commit modes for the managed tx methods\\n    \"\"\"\\n\\n    NONE = 0\\n    FLUSH = 1\\n    COMMIT = 2\\n    ROLLBACK = 3']\n",
            "len of class: 146\n",
            "5 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/language_classification.py\n",
            "Imports: ['import os', 'import pickle', 'from collections import Counter', 'from sklearn import metrics', 'from sklearn.feature_extraction.text import TfidfVectorizer', 'from sklearn.model_selection import train_test_split', 'from sklearn.pipeline import Pipeline', 'from sklearn.svm import LinearSVC']\n",
            "functions: ['def load_and_split(foldername, num_words):\\n    ls = os.listdir(foldername)\\n    X = []\\n    Y = []\\n    langmap = dict()\\n    for idx, x in enumerate(ls):\\n        print(\"loading language\", x)\\n        with open(foldername + \"/\" + x, \"r\") as reader:\\n            tmp = reader.read().split(\" \")\\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\\n            X.extend(tmp)\\n            Y.extend([idx] * len(tmp))\\n            langmap[idx] = x\\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\\n    return x_train, x_test, y_train, y_test, langmap', 'def build_and_train_pipeline(x_train, y_train):\\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\\n    clf = Pipeline(\\n        [\\n            (\"vec\", vectorizer),\\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\\n            (\"clf\", LinearSVC(C=0.5)),\\n            # (\"clf\",GaussianNB())\\n            # (\"clf\", HistGradientBoostingClassifier())\\n        ]\\n    )\\n    print(\"fitting model...\")\\n    clf.fit(x_train, y_train)\\n    return clf', 'def benchmark(clf, x_test, y_test, langmap):\\n    print(\"benchmarking model...\")\\n    y_pred = clf.predict(x_test)\\n    names = list(langmap.values())\\n    # print(y_test)\\n    # print(langmap)\\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\\n    cm = metrics.confusion_matrix(y_test, y_pred)\\n    print(cm)', 'def main(foldername, modelname, num_words):\\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\\n    clf = build_and_train_pipeline(x_train, y_train)\\n    benchmark(clf, x_test, y_test, langmap)\\n    save_model(clf, langmap, num_words, modelname)\\n    model = load(modelname)\\n    print(\\n        \"running infernence on long tests\",\\n        inference_voter(\\n            model,\\n            \"\"\"\\n    What language is this text written in? Nobody knows until you fill in at least ten words.\\n    This test here is to check whether the moving window approach works,\\n    so I still need to fill in a little more text.\\n    \"\"\",\\n        ),\\n    )', 'def load(modelname):\\n    with open(modelname, \"rb\") as writer:\\n        data = pickle.load(writer)\\n    return data', 'def save_model(model, idx_to_name, num_words, modelname):\\n    out = {\\n        \"model\": model,\\n        \"idx_to_name\": idx_to_name,\\n        \"num_words\": num_words,\\n    }\\n    with open(modelname, \"wb\") as writer:\\n        pickle.dump(out, writer)', 'def inference_voter(model, text):\\n    tmp = text.split()\\n    # print(len(tmp), tmp)\\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\\n    predictions = model[\"model\"].predict(tmp)\\n    # print(\"integer predictions\", predictions)\\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\\n    result = Counter(predictions).most_common(1)[0][0]\\n    return model[\"idx_to_name\"][result]']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/language_classification.py\n",
            "Imports: ['import os', 'import pickle', 'from collections import Counter', 'from sklearn import metrics', 'from sklearn.feature_extraction.text import TfidfVectorizer', 'from sklearn.model_selection import train_test_split', 'from sklearn.pipeline import Pipeline', 'from sklearn.svm import LinearSVC']\n",
            "functions: ['def load_and_split(foldername, num_words):\\n    ls = os.listdir(foldername)\\n    X = []\\n    Y = []\\n    langmap = dict()\\n    for idx, x in enumerate(ls):\\n        print(\"loading language\", x)\\n        with open(foldername + \"/\" + x, \"r\") as reader:\\n            tmp = reader.read().split(\" \")\\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\\n            X.extend(tmp)\\n            Y.extend([idx] * len(tmp))\\n            langmap[idx] = x\\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\\n    return x_train, x_test, y_train, y_test, langmap', 'def build_and_train_pipeline(x_train, y_train):\\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\\n    clf = Pipeline(\\n        [\\n            (\"vec\", vectorizer),\\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\\n            (\"clf\", LinearSVC(C=0.5)),\\n            # (\"clf\",GaussianNB())\\n            # (\"clf\", HistGradientBoostingClassifier())\\n        ]\\n    )\\n    print(\"fitting model...\")\\n    clf.fit(x_train, y_train)\\n    return clf', 'def benchmark(clf, x_test, y_test, langmap):\\n    print(\"benchmarking model...\")\\n    y_pred = clf.predict(x_test)\\n    names = list(langmap.values())\\n    # print(y_test)\\n    # print(langmap)\\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\\n    cm = metrics.confusion_matrix(y_test, y_pred)\\n    print(cm)', 'def main(foldername, modelname, num_words):\\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\\n    clf = build_and_train_pipeline(x_train, y_train)\\n    benchmark(clf, x_test, y_test, langmap)\\n    save_model(clf, langmap, num_words, modelname)\\n    model = load(modelname)\\n    print(\\n        \"running infernence on long tests\",\\n        inference_voter(\\n            model,\\n            \"\"\"\\n    What language is this text written in? Nobody knows until you fill in at least ten words.\\n    This test here is to check whether the moving window approach works,\\n    so I still need to fill in a little more text.\\n    \"\"\",\\n        ),\\n    )', 'def load(modelname):\\n    with open(modelname, \"rb\") as writer:\\n        data = pickle.load(writer)\\n    return data', 'def save_model(model, idx_to_name, num_words, modelname):\\n    out = {\\n        \"model\": model,\\n        \"idx_to_name\": idx_to_name,\\n        \"num_words\": num_words,\\n    }\\n    with open(modelname, \"wb\") as writer:\\n        pickle.dump(out, writer)', 'def inference_voter(model, text):\\n    tmp = text.split()\\n    # print(len(tmp), tmp)\\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\\n    predictions = model[\"model\"].predict(tmp)\\n    # print(\"integer predictions\", predictions)\\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\\n    result = Counter(predictions).most_common(1)[0][0]\\n    return model[\"idx_to_name\"][result]']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/language_classification.py\n",
            "Imports: ['import os', 'import pickle', 'from collections import Counter', 'from sklearn import metrics', 'from sklearn.feature_extraction.text import TfidfVectorizer', 'from sklearn.model_selection import train_test_split', 'from sklearn.pipeline import Pipeline', 'from sklearn.svm import LinearSVC']\n",
            "functions: ['def load_and_split(foldername, num_words):\\n    ls = os.listdir(foldername)\\n    X = []\\n    Y = []\\n    langmap = dict()\\n    for idx, x in enumerate(ls):\\n        print(\"loading language\", x)\\n        with open(foldername + \"/\" + x, \"r\") as reader:\\n            tmp = reader.read().split(\" \")\\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\\n            X.extend(tmp)\\n            Y.extend([idx] * len(tmp))\\n            langmap[idx] = x\\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\\n    return x_train, x_test, y_train, y_test, langmap', 'def build_and_train_pipeline(x_train, y_train):\\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\\n    clf = Pipeline(\\n        [\\n            (\"vec\", vectorizer),\\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\\n            (\"clf\", LinearSVC(C=0.5)),\\n            # (\"clf\",GaussianNB())\\n            # (\"clf\", HistGradientBoostingClassifier())\\n        ]\\n    )\\n    print(\"fitting model...\")\\n    clf.fit(x_train, y_train)\\n    return clf', 'def benchmark(clf, x_test, y_test, langmap):\\n    print(\"benchmarking model...\")\\n    y_pred = clf.predict(x_test)\\n    names = list(langmap.values())\\n    # print(y_test)\\n    # print(langmap)\\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\\n    cm = metrics.confusion_matrix(y_test, y_pred)\\n    print(cm)', 'def main(foldername, modelname, num_words):\\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\\n    clf = build_and_train_pipeline(x_train, y_train)\\n    benchmark(clf, x_test, y_test, langmap)\\n    save_model(clf, langmap, num_words, modelname)\\n    model = load(modelname)\\n    print(\\n        \"running infernence on long tests\",\\n        inference_voter(\\n            model,\\n            \"\"\"\\n    What language is this text written in? Nobody knows until you fill in at least ten words.\\n    This test here is to check whether the moving window approach works,\\n    so I still need to fill in a little more text.\\n    \"\"\",\\n        ),\\n    )', 'def load(modelname):\\n    with open(modelname, \"rb\") as writer:\\n        data = pickle.load(writer)\\n    return data', 'def save_model(model, idx_to_name, num_words, modelname):\\n    out = {\\n        \"model\": model,\\n        \"idx_to_name\": idx_to_name,\\n        \"num_words\": num_words,\\n    }\\n    with open(modelname, \"wb\") as writer:\\n        pickle.dump(out, writer)', 'def inference_voter(model, text):\\n    tmp = text.split()\\n    # print(len(tmp), tmp)\\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\\n    predictions = model[\"model\"].predict(tmp)\\n    # print(\"integer predictions\", predictions)\\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\\n    result = Counter(predictions).most_common(1)[0][0]\\n    return model[\"idx_to_name\"][result]']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/language_classification.py\n",
            "Imports: ['import os', 'import pickle', 'from collections import Counter', 'from sklearn import metrics', 'from sklearn.feature_extraction.text import TfidfVectorizer', 'from sklearn.model_selection import train_test_split', 'from sklearn.pipeline import Pipeline', 'from sklearn.svm import LinearSVC']\n",
            "functions: ['def load_and_split(foldername, num_words):\\n    ls = os.listdir(foldername)\\n    X = []\\n    Y = []\\n    langmap = dict()\\n    for idx, x in enumerate(ls):\\n        print(\"loading language\", x)\\n        with open(foldername + \"/\" + x, \"r\") as reader:\\n            tmp = reader.read().split(\" \")\\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\\n            X.extend(tmp)\\n            Y.extend([idx] * len(tmp))\\n            langmap[idx] = x\\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\\n    return x_train, x_test, y_train, y_test, langmap', 'def build_and_train_pipeline(x_train, y_train):\\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\\n    clf = Pipeline(\\n        [\\n            (\"vec\", vectorizer),\\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\\n            (\"clf\", LinearSVC(C=0.5)),\\n            # (\"clf\",GaussianNB())\\n            # (\"clf\", HistGradientBoostingClassifier())\\n        ]\\n    )\\n    print(\"fitting model...\")\\n    clf.fit(x_train, y_train)\\n    return clf', 'def benchmark(clf, x_test, y_test, langmap):\\n    print(\"benchmarking model...\")\\n    y_pred = clf.predict(x_test)\\n    names = list(langmap.values())\\n    # print(y_test)\\n    # print(langmap)\\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\\n    cm = metrics.confusion_matrix(y_test, y_pred)\\n    print(cm)', 'def main(foldername, modelname, num_words):\\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\\n    clf = build_and_train_pipeline(x_train, y_train)\\n    benchmark(clf, x_test, y_test, langmap)\\n    save_model(clf, langmap, num_words, modelname)\\n    model = load(modelname)\\n    print(\\n        \"running infernence on long tests\",\\n        inference_voter(\\n            model,\\n            \"\"\"\\n    What language is this text written in? Nobody knows until you fill in at least ten words.\\n    This test here is to check whether the moving window approach works,\\n    so I still need to fill in a little more text.\\n    \"\"\",\\n        ),\\n    )', 'def load(modelname):\\n    with open(modelname, \"rb\") as writer:\\n        data = pickle.load(writer)\\n    return data', 'def save_model(model, idx_to_name, num_words, modelname):\\n    out = {\\n        \"model\": model,\\n        \"idx_to_name\": idx_to_name,\\n        \"num_words\": num_words,\\n    }\\n    with open(modelname, \"wb\") as writer:\\n        pickle.dump(out, writer)', 'def inference_voter(model, text):\\n    tmp = text.split()\\n    # print(len(tmp), tmp)\\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\\n    predictions = model[\"model\"].predict(tmp)\\n    # print(\"integer predictions\", predictions)\\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\\n    result = Counter(predictions).most_common(1)[0][0]\\n    return model[\"idx_to_name\"][result]']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/language_classification.py\n",
            "Imports: ['import os', 'import pickle', 'from collections import Counter', 'from sklearn import metrics', 'from sklearn.feature_extraction.text import TfidfVectorizer', 'from sklearn.model_selection import train_test_split', 'from sklearn.pipeline import Pipeline', 'from sklearn.svm import LinearSVC']\n",
            "functions: ['def load_and_split(foldername, num_words):\\n    ls = os.listdir(foldername)\\n    X = []\\n    Y = []\\n    langmap = dict()\\n    for idx, x in enumerate(ls):\\n        print(\"loading language\", x)\\n        with open(foldername + \"/\" + x, \"r\") as reader:\\n            tmp = reader.read().split(\" \")\\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\\n            X.extend(tmp)\\n            Y.extend([idx] * len(tmp))\\n            langmap[idx] = x\\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\\n    return x_train, x_test, y_train, y_test, langmap', 'def build_and_train_pipeline(x_train, y_train):\\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\\n    clf = Pipeline(\\n        [\\n            (\"vec\", vectorizer),\\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\\n            (\"clf\", LinearSVC(C=0.5)),\\n            # (\"clf\",GaussianNB())\\n            # (\"clf\", HistGradientBoostingClassifier())\\n        ]\\n    )\\n    print(\"fitting model...\")\\n    clf.fit(x_train, y_train)\\n    return clf', 'def benchmark(clf, x_test, y_test, langmap):\\n    print(\"benchmarking model...\")\\n    y_pred = clf.predict(x_test)\\n    names = list(langmap.values())\\n    # print(y_test)\\n    # print(langmap)\\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\\n    cm = metrics.confusion_matrix(y_test, y_pred)\\n    print(cm)', 'def main(foldername, modelname, num_words):\\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\\n    clf = build_and_train_pipeline(x_train, y_train)\\n    benchmark(clf, x_test, y_test, langmap)\\n    save_model(clf, langmap, num_words, modelname)\\n    model = load(modelname)\\n    print(\\n        \"running infernence on long tests\",\\n        inference_voter(\\n            model,\\n            \"\"\"\\n    What language is this text written in? Nobody knows until you fill in at least ten words.\\n    This test here is to check whether the moving window approach works,\\n    so I still need to fill in a little more text.\\n    \"\"\",\\n        ),\\n    )', 'def load(modelname):\\n    with open(modelname, \"rb\") as writer:\\n        data = pickle.load(writer)\\n    return data', 'def save_model(model, idx_to_name, num_words, modelname):\\n    out = {\\n        \"model\": model,\\n        \"idx_to_name\": idx_to_name,\\n        \"num_words\": num_words,\\n    }\\n    with open(modelname, \"wb\") as writer:\\n        pickle.dump(out, writer)', 'def inference_voter(model, text):\\n    tmp = text.split()\\n    # print(len(tmp), tmp)\\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\\n    predictions = model[\"model\"].predict(tmp)\\n    # print(\"integer predictions\", predictions)\\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\\n    result = Counter(predictions).most_common(1)[0][0]\\n    return model[\"idx_to_name\"][result]']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/language_classification.py\n",
            "Imports: ['import os', 'import pickle', 'from collections import Counter', 'from sklearn import metrics', 'from sklearn.feature_extraction.text import TfidfVectorizer', 'from sklearn.model_selection import train_test_split', 'from sklearn.pipeline import Pipeline', 'from sklearn.svm import LinearSVC']\n",
            "functions: ['def load_and_split(foldername, num_words):\\n    ls = os.listdir(foldername)\\n    X = []\\n    Y = []\\n    langmap = dict()\\n    for idx, x in enumerate(ls):\\n        print(\"loading language\", x)\\n        with open(foldername + \"/\" + x, \"r\") as reader:\\n            tmp = reader.read().split(\" \")\\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\\n            X.extend(tmp)\\n            Y.extend([idx] * len(tmp))\\n            langmap[idx] = x\\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\\n    return x_train, x_test, y_train, y_test, langmap', 'def build_and_train_pipeline(x_train, y_train):\\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\\n    clf = Pipeline(\\n        [\\n            (\"vec\", vectorizer),\\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\\n            (\"clf\", LinearSVC(C=0.5)),\\n            # (\"clf\",GaussianNB())\\n            # (\"clf\", HistGradientBoostingClassifier())\\n        ]\\n    )\\n    print(\"fitting model...\")\\n    clf.fit(x_train, y_train)\\n    return clf', 'def benchmark(clf, x_test, y_test, langmap):\\n    print(\"benchmarking model...\")\\n    y_pred = clf.predict(x_test)\\n    names = list(langmap.values())\\n    # print(y_test)\\n    # print(langmap)\\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\\n    cm = metrics.confusion_matrix(y_test, y_pred)\\n    print(cm)', 'def main(foldername, modelname, num_words):\\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\\n    clf = build_and_train_pipeline(x_train, y_train)\\n    benchmark(clf, x_test, y_test, langmap)\\n    save_model(clf, langmap, num_words, modelname)\\n    model = load(modelname)\\n    print(\\n        \"running infernence on long tests\",\\n        inference_voter(\\n            model,\\n            \"\"\"\\n    What language is this text written in? Nobody knows until you fill in at least ten words.\\n    This test here is to check whether the moving window approach works,\\n    so I still need to fill in a little more text.\\n    \"\"\",\\n        ),\\n    )', 'def load(modelname):\\n    with open(modelname, \"rb\") as writer:\\n        data = pickle.load(writer)\\n    return data', 'def save_model(model, idx_to_name, num_words, modelname):\\n    out = {\\n        \"model\": model,\\n        \"idx_to_name\": idx_to_name,\\n        \"num_words\": num_words,\\n    }\\n    with open(modelname, \"wb\") as writer:\\n        pickle.dump(out, writer)', 'def inference_voter(model, text):\\n    tmp = text.split()\\n    # print(len(tmp), tmp)\\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\\n    predictions = model[\"model\"].predict(tmp)\\n    # print(\"integer predictions\", predictions)\\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\\n    result = Counter(predictions).most_common(1)[0][0]\\n    return model[\"idx_to_name\"][result]']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/hugging_face.py\n",
            "Imports: ['from enum import Enum', 'from typing import Any, Dict', 'import aiohttp', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_shared.exceptions import OasstError, OasstErrorCode']\n",
            "functions: []\n",
            "Classes: ['class HfUrl(str, Enum):\\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"', 'class HfClassificationModel(str, Enum):\\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"', 'class HfEmbeddingModel(str, Enum):\\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"', 'class HuggingFaceAPI:\\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\\n\\n    def __init__(\\n        self,\\n        api_url: str,\\n    ):\\n        # The API endpoint we want to access\\n        self.api_url: str = api_url\\n\\n        # Access token for the api\\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\\n\\n        # Headers going to be used\\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\\n\\n    async def post(self, input: str) -> Any:\\n        \"\"\"Post request to the endpoint to get an inference\\n\\n        Args:\\n            input (str): the input that we will pass to the model\\n\\n        Raises:\\n            OasstError: in the case we get a bad response\\n\\n        Returns:\\n            inference: the inference we obtain from the model in HF\\n        \"\"\"\\n\\n        async with aiohttp.ClientSession() as session:\\n            payload: Dict[str, str] = {\"inputs\": input}\\n\\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\\n                # If we get a bad response\\n                if not response.ok:\\n                    logger.error(response)\\n                    logger.info(self.headers)\\n                    raise OasstError(\\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\\n                    )\\n\\n                # Get the response from the API call\\n                inference = await response.json()\\n\\n        return inference']\n",
            "len of class: 212\n",
            "len of class: 100\n",
            "len of class: 109\n",
            "len of class: 1568\n",
            "0 4\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/hugging_face.py\n",
            "Imports: ['from enum import Enum', 'from typing import Any, Dict', 'import aiohttp', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_shared.exceptions import OasstError, OasstErrorCode']\n",
            "functions: []\n",
            "Classes: ['class HfUrl(str, Enum):\\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"', 'class HfClassificationModel(str, Enum):\\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"', 'class HfEmbeddingModel(str, Enum):\\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"', 'class HuggingFaceAPI:\\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\\n\\n    def __init__(\\n        self,\\n        api_url: str,\\n    ):\\n        # The API endpoint we want to access\\n        self.api_url: str = api_url\\n\\n        # Access token for the api\\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\\n\\n        # Headers going to be used\\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\\n\\n    async def post(self, input: str) -> Any:\\n        \"\"\"Post request to the endpoint to get an inference\\n\\n        Args:\\n            input (str): the input that we will pass to the model\\n\\n        Raises:\\n            OasstError: in the case we get a bad response\\n\\n        Returns:\\n            inference: the inference we obtain from the model in HF\\n        \"\"\"\\n\\n        async with aiohttp.ClientSession() as session:\\n            payload: Dict[str, str] = {\"inputs\": input}\\n\\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\\n                # If we get a bad response\\n                if not response.ok:\\n                    logger.error(response)\\n                    logger.info(self.headers)\\n                    raise OasstError(\\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\\n                    )\\n\\n                # Get the response from the API call\\n                inference = await response.json()\\n\\n        return inference']\n",
            "len of class: 212\n",
            "len of class: 100\n",
            "len of class: 109\n",
            "len of class: 1568\n",
            "0 4\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/hugging_face.py\n",
            "Imports: ['from enum import Enum', 'from typing import Any, Dict', 'import aiohttp', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_shared.exceptions import OasstError, OasstErrorCode']\n",
            "functions: []\n",
            "Classes: ['class HfUrl(str, Enum):\\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"', 'class HfClassificationModel(str, Enum):\\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"', 'class HfEmbeddingModel(str, Enum):\\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"', 'class HuggingFaceAPI:\\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\\n\\n    def __init__(\\n        self,\\n        api_url: str,\\n    ):\\n        # The API endpoint we want to access\\n        self.api_url: str = api_url\\n\\n        # Access token for the api\\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\\n\\n        # Headers going to be used\\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\\n\\n    async def post(self, input: str) -> Any:\\n        \"\"\"Post request to the endpoint to get an inference\\n\\n        Args:\\n            input (str): the input that we will pass to the model\\n\\n        Raises:\\n            OasstError: in the case we get a bad response\\n\\n        Returns:\\n            inference: the inference we obtain from the model in HF\\n        \"\"\"\\n\\n        async with aiohttp.ClientSession() as session:\\n            payload: Dict[str, str] = {\"inputs\": input}\\n\\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\\n                # If we get a bad response\\n                if not response.ok:\\n                    logger.error(response)\\n                    logger.info(self.headers)\\n                    raise OasstError(\\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\\n                    )\\n\\n                # Get the response from the API call\\n                inference = await response.json()\\n\\n        return inference']\n",
            "len of class: 212\n",
            "len of class: 100\n",
            "len of class: 109\n",
            "len of class: 1568\n",
            "0 4\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/hugging_face.py\n",
            "Imports: ['from enum import Enum', 'from typing import Any, Dict', 'import aiohttp', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_shared.exceptions import OasstError, OasstErrorCode']\n",
            "functions: []\n",
            "Classes: ['class HfUrl(str, Enum):\\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"', 'class HfClassificationModel(str, Enum):\\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"', 'class HfEmbeddingModel(str, Enum):\\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"', 'class HuggingFaceAPI:\\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\\n\\n    def __init__(\\n        self,\\n        api_url: str,\\n    ):\\n        # The API endpoint we want to access\\n        self.api_url: str = api_url\\n\\n        # Access token for the api\\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\\n\\n        # Headers going to be used\\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\\n\\n    async def post(self, input: str) -> Any:\\n        \"\"\"Post request to the endpoint to get an inference\\n\\n        Args:\\n            input (str): the input that we will pass to the model\\n\\n        Raises:\\n            OasstError: in the case we get a bad response\\n\\n        Returns:\\n            inference: the inference we obtain from the model in HF\\n        \"\"\"\\n\\n        async with aiohttp.ClientSession() as session:\\n            payload: Dict[str, str] = {\"inputs\": input}\\n\\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\\n                # If we get a bad response\\n                if not response.ok:\\n                    logger.error(response)\\n                    logger.info(self.headers)\\n                    raise OasstError(\\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\\n                    )\\n\\n                # Get the response from the API call\\n                inference = await response.json()\\n\\n        return inference']\n",
            "len of class: 212\n",
            "len of class: 100\n",
            "len of class: 109\n",
            "len of class: 1568\n",
            "0 4\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/hugging_face.py\n",
            "Imports: ['from enum import Enum', 'from typing import Any, Dict', 'import aiohttp', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_shared.exceptions import OasstError, OasstErrorCode']\n",
            "functions: []\n",
            "Classes: ['class HfUrl(str, Enum):\\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"', 'class HfClassificationModel(str, Enum):\\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"', 'class HfEmbeddingModel(str, Enum):\\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"', 'class HuggingFaceAPI:\\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\\n\\n    def __init__(\\n        self,\\n        api_url: str,\\n    ):\\n        # The API endpoint we want to access\\n        self.api_url: str = api_url\\n\\n        # Access token for the api\\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\\n\\n        # Headers going to be used\\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\\n\\n    async def post(self, input: str) -> Any:\\n        \"\"\"Post request to the endpoint to get an inference\\n\\n        Args:\\n            input (str): the input that we will pass to the model\\n\\n        Raises:\\n            OasstError: in the case we get a bad response\\n\\n        Returns:\\n            inference: the inference we obtain from the model in HF\\n        \"\"\"\\n\\n        async with aiohttp.ClientSession() as session:\\n            payload: Dict[str, str] = {\"inputs\": input}\\n\\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\\n                # If we get a bad response\\n                if not response.ok:\\n                    logger.error(response)\\n                    logger.info(self.headers)\\n                    raise OasstError(\\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\\n                    )\\n\\n                # Get the response from the API call\\n                inference = await response.json()\\n\\n        return inference']\n",
            "len of class: 212\n",
            "len of class: 100\n",
            "len of class: 109\n",
            "len of class: 1568\n",
            "0 4\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/hugging_face.py\n",
            "Imports: ['from enum import Enum', 'from typing import Any, Dict', 'import aiohttp', 'from loguru import logger', 'from oasst_backend.config import settings', 'from oasst_shared.exceptions import OasstError, OasstErrorCode']\n",
            "functions: []\n",
            "Classes: ['class HfUrl(str, Enum):\\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"', 'class HfClassificationModel(str, Enum):\\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"', 'class HfEmbeddingModel(str, Enum):\\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"', 'class HuggingFaceAPI:\\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\\n\\n    def __init__(\\n        self,\\n        api_url: str,\\n    ):\\n        # The API endpoint we want to access\\n        self.api_url: str = api_url\\n\\n        # Access token for the api\\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\\n\\n        # Headers going to be used\\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\\n\\n    async def post(self, input: str) -> Any:\\n        \"\"\"Post request to the endpoint to get an inference\\n\\n        Args:\\n            input (str): the input that we will pass to the model\\n\\n        Raises:\\n            OasstError: in the case we get a bad response\\n\\n        Returns:\\n            inference: the inference we obtain from the model in HF\\n        \"\"\"\\n\\n        async with aiohttp.ClientSession() as session:\\n            payload: Dict[str, str] = {\"inputs\": input}\\n\\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\\n                # If we get a bad response\\n                if not response.ok:\\n                    logger.error(response)\\n                    logger.info(self.headers)\\n                    raise OasstError(\\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\\n                    )\\n\\n                # Get the response from the API call\\n                inference = await response.json()\\n\\n        return inference']\n",
            "len of class: 212\n",
            "len of class: 100\n",
            "len of class: 109\n",
            "len of class: 1568\n",
            "0 4\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/tree_export.py\n",
            "Imports: ['from __future__ import annotations', 'import contextlib', 'import gzip', 'import hashlib', 'import json', 'import sys', 'import uuid', 'from collections import defaultdict', 'from typing import Iterable, Optional, TextIO', 'from fastapi.encoders import jsonable_encoder', 'from oasst_backend.models import Message', 'from oasst_backend.models.message_tree_state import State as TreeState', 'from oasst_shared.schemas.export import ExportMessageEvent, ExportMessageEventEmoji, ExportMessageEventRanking, ExportMessageEventRating, ExportMessageNode, ExportMessageTree, LabelValues']\n",
            "functions: ['def sha256_hash(key: str, seed: int) -> str:\\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()', 'def prepare_export_message_node(\\n    message: Message,\\n    labels: Optional[LabelValues] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[str, list[ExportMessageEvent]] | None = None,\\n) -> ExportMessageNode:\\n    message_id = str(message.id)\\n    parent_id = str(message.parent_id) if message.parent_id else None\\n    user_id = str(message.user_id) if message.user_id else None\\n    if anonymizer is not None:\\n        message_id = anonymizer.anonymize(\"message\", message_id)\\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\\n        user_id = anonymizer.anonymize(\"user\", user_id)\\n        if events is not None:\\n            for event_key, event_values in events.items():\\n                for event in event_values:\\n                    match event_key:\\n                        case \"emoji\":\\n                            event: ExportMessageEventEmoji = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"rating\":\\n                            event: ExportMessageEventRating = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"ranking\":\\n                            event: ExportMessageEventRanking = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                            event.ranked_message_ids = [\\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\\n                            ]\\n                            if event.ranking_parent_id is not None:\\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\\n                            if event.message_tree_id is not None:\\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\\n                        case _:\\n                            raise ValueError(f\"Unknown event type {event_key}\")\\n    assert message_id is not None\\n    return ExportMessageNode(\\n        message_id=message_id,\\n        parent_id=parent_id,\\n        user_id=user_id,\\n        text=str(message.payload.payload.text),\\n        role=message.role,\\n        lang=message.lang,\\n        deleted=message.deleted,\\n        review_count=message.review_count,\\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\\n        synthetic=message.synthetic,\\n        model_name=message.model_name,\\n        emojis=message.emojis,\\n        rank=message.rank,\\n        labels=labels,\\n        events=events,\\n    )', 'def build_export_tree(\\n    message_tree_id: uuid.UUID,\\n    message_tree_state: TreeState,\\n    messages: list[Message],\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> ExportMessageTree:\\n    export_messages = [\\n        prepare_export_message_node(\\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n        )\\n        for m in messages\\n    ]\\n\\n    messages_by_parent = defaultdict(list)\\n    for message in export_messages:\\n        messages_by_parent[message.parent_id].append(message)\\n\\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\\n        node.replies = messages_by_parent[node.message_id]\\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\\n        for child in node.replies:\\n            assign_replies(child)\\n        return node\\n\\n    prompt = assign_replies(messages_by_parent[None][0])\\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)', 'def smart_open(filename: str = None) -> TextIO:\\n    if filename and filename != \"-\":\\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        fh = sys.stdout\\n\\n    try:\\n        yield fh\\n    finally:\\n        if fh is not sys.stdout:\\n            fh.close()', 'def write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for tree in trees:\\n            file_data = jsonable_encoder(tree, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")', 'def write_messages_to_file(\\n    filename: str | None,\\n    messages: Iterable[Message],\\n    use_compression: bool = True,\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for m in messages:\\n            export_message = prepare_export_message_node(\\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n            )\\n\\n            file_data = jsonable_encoder(export_message, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")']\n",
            "Classes: ['class Anonymizer:\\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\\n        self._map = {}\\n        self._values = set()\\n        self._seed = seed\\n        self._gen = value_generator\\n\\n    def __getitem__(self, key):\\n        if key not in self._map:\\n            new_value = self._gen(key, self._seed)\\n            if new_value in self._values:\\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\\n            self._map[key] = new_value\\n            self._values.add(new_value)\\n        return self._map[key]\\n\\n    def anonymize(self, collection: str, key: str | None) -> str | None:\\n        if key is None:\\n            return None\\n        return self[f\"{collection}:{key}\"]']\n",
            "len of class: 761\n",
            "6 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/tree_export.py\n",
            "Imports: ['from __future__ import annotations', 'import contextlib', 'import gzip', 'import hashlib', 'import json', 'import sys', 'import uuid', 'from collections import defaultdict', 'from typing import Iterable, Optional, TextIO', 'from fastapi.encoders import jsonable_encoder', 'from oasst_backend.models import Message', 'from oasst_backend.models.message_tree_state import State as TreeState', 'from oasst_shared.schemas.export import ExportMessageEvent, ExportMessageEventEmoji, ExportMessageEventRanking, ExportMessageEventRating, ExportMessageNode, ExportMessageTree, LabelValues']\n",
            "functions: ['def sha256_hash(key: str, seed: int) -> str:\\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()', 'def prepare_export_message_node(\\n    message: Message,\\n    labels: Optional[LabelValues] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[str, list[ExportMessageEvent]] | None = None,\\n) -> ExportMessageNode:\\n    message_id = str(message.id)\\n    parent_id = str(message.parent_id) if message.parent_id else None\\n    user_id = str(message.user_id) if message.user_id else None\\n    if anonymizer is not None:\\n        message_id = anonymizer.anonymize(\"message\", message_id)\\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\\n        user_id = anonymizer.anonymize(\"user\", user_id)\\n        if events is not None:\\n            for event_key, event_values in events.items():\\n                for event in event_values:\\n                    match event_key:\\n                        case \"emoji\":\\n                            event: ExportMessageEventEmoji = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"rating\":\\n                            event: ExportMessageEventRating = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"ranking\":\\n                            event: ExportMessageEventRanking = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                            event.ranked_message_ids = [\\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\\n                            ]\\n                            if event.ranking_parent_id is not None:\\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\\n                            if event.message_tree_id is not None:\\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\\n                        case _:\\n                            raise ValueError(f\"Unknown event type {event_key}\")\\n    assert message_id is not None\\n    return ExportMessageNode(\\n        message_id=message_id,\\n        parent_id=parent_id,\\n        user_id=user_id,\\n        text=str(message.payload.payload.text),\\n        role=message.role,\\n        lang=message.lang,\\n        deleted=message.deleted,\\n        review_count=message.review_count,\\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\\n        synthetic=message.synthetic,\\n        model_name=message.model_name,\\n        emojis=message.emojis,\\n        rank=message.rank,\\n        labels=labels,\\n        events=events,\\n    )', 'def build_export_tree(\\n    message_tree_id: uuid.UUID,\\n    message_tree_state: TreeState,\\n    messages: list[Message],\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> ExportMessageTree:\\n    export_messages = [\\n        prepare_export_message_node(\\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n        )\\n        for m in messages\\n    ]\\n\\n    messages_by_parent = defaultdict(list)\\n    for message in export_messages:\\n        messages_by_parent[message.parent_id].append(message)\\n\\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\\n        node.replies = messages_by_parent[node.message_id]\\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\\n        for child in node.replies:\\n            assign_replies(child)\\n        return node\\n\\n    prompt = assign_replies(messages_by_parent[None][0])\\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)', 'def smart_open(filename: str = None) -> TextIO:\\n    if filename and filename != \"-\":\\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        fh = sys.stdout\\n\\n    try:\\n        yield fh\\n    finally:\\n        if fh is not sys.stdout:\\n            fh.close()', 'def write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for tree in trees:\\n            file_data = jsonable_encoder(tree, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")', 'def write_messages_to_file(\\n    filename: str | None,\\n    messages: Iterable[Message],\\n    use_compression: bool = True,\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for m in messages:\\n            export_message = prepare_export_message_node(\\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n            )\\n\\n            file_data = jsonable_encoder(export_message, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")']\n",
            "Classes: ['class Anonymizer:\\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\\n        self._map = {}\\n        self._values = set()\\n        self._seed = seed\\n        self._gen = value_generator\\n\\n    def __getitem__(self, key):\\n        if key not in self._map:\\n            new_value = self._gen(key, self._seed)\\n            if new_value in self._values:\\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\\n            self._map[key] = new_value\\n            self._values.add(new_value)\\n        return self._map[key]\\n\\n    def anonymize(self, collection: str, key: str | None) -> str | None:\\n        if key is None:\\n            return None\\n        return self[f\"{collection}:{key}\"]']\n",
            "len of class: 761\n",
            "6 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/tree_export.py\n",
            "Imports: ['from __future__ import annotations', 'import contextlib', 'import gzip', 'import hashlib', 'import json', 'import sys', 'import uuid', 'from collections import defaultdict', 'from typing import Iterable, Optional, TextIO', 'from fastapi.encoders import jsonable_encoder', 'from oasst_backend.models import Message', 'from oasst_backend.models.message_tree_state import State as TreeState', 'from oasst_shared.schemas.export import ExportMessageEvent, ExportMessageEventEmoji, ExportMessageEventRanking, ExportMessageEventRating, ExportMessageNode, ExportMessageTree, LabelValues']\n",
            "functions: ['def sha256_hash(key: str, seed: int) -> str:\\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()', 'def prepare_export_message_node(\\n    message: Message,\\n    labels: Optional[LabelValues] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[str, list[ExportMessageEvent]] | None = None,\\n) -> ExportMessageNode:\\n    message_id = str(message.id)\\n    parent_id = str(message.parent_id) if message.parent_id else None\\n    user_id = str(message.user_id) if message.user_id else None\\n    if anonymizer is not None:\\n        message_id = anonymizer.anonymize(\"message\", message_id)\\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\\n        user_id = anonymizer.anonymize(\"user\", user_id)\\n        if events is not None:\\n            for event_key, event_values in events.items():\\n                for event in event_values:\\n                    match event_key:\\n                        case \"emoji\":\\n                            event: ExportMessageEventEmoji = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"rating\":\\n                            event: ExportMessageEventRating = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"ranking\":\\n                            event: ExportMessageEventRanking = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                            event.ranked_message_ids = [\\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\\n                            ]\\n                            if event.ranking_parent_id is not None:\\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\\n                            if event.message_tree_id is not None:\\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\\n                        case _:\\n                            raise ValueError(f\"Unknown event type {event_key}\")\\n    assert message_id is not None\\n    return ExportMessageNode(\\n        message_id=message_id,\\n        parent_id=parent_id,\\n        user_id=user_id,\\n        text=str(message.payload.payload.text),\\n        role=message.role,\\n        lang=message.lang,\\n        deleted=message.deleted,\\n        review_count=message.review_count,\\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\\n        synthetic=message.synthetic,\\n        model_name=message.model_name,\\n        emojis=message.emojis,\\n        rank=message.rank,\\n        labels=labels,\\n        events=events,\\n    )', 'def build_export_tree(\\n    message_tree_id: uuid.UUID,\\n    message_tree_state: TreeState,\\n    messages: list[Message],\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> ExportMessageTree:\\n    export_messages = [\\n        prepare_export_message_node(\\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n        )\\n        for m in messages\\n    ]\\n\\n    messages_by_parent = defaultdict(list)\\n    for message in export_messages:\\n        messages_by_parent[message.parent_id].append(message)\\n\\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\\n        node.replies = messages_by_parent[node.message_id]\\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\\n        for child in node.replies:\\n            assign_replies(child)\\n        return node\\n\\n    prompt = assign_replies(messages_by_parent[None][0])\\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)', 'def smart_open(filename: str = None) -> TextIO:\\n    if filename and filename != \"-\":\\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        fh = sys.stdout\\n\\n    try:\\n        yield fh\\n    finally:\\n        if fh is not sys.stdout:\\n            fh.close()', 'def write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for tree in trees:\\n            file_data = jsonable_encoder(tree, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")', 'def write_messages_to_file(\\n    filename: str | None,\\n    messages: Iterable[Message],\\n    use_compression: bool = True,\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for m in messages:\\n            export_message = prepare_export_message_node(\\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n            )\\n\\n            file_data = jsonable_encoder(export_message, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")']\n",
            "Classes: ['class Anonymizer:\\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\\n        self._map = {}\\n        self._values = set()\\n        self._seed = seed\\n        self._gen = value_generator\\n\\n    def __getitem__(self, key):\\n        if key not in self._map:\\n            new_value = self._gen(key, self._seed)\\n            if new_value in self._values:\\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\\n            self._map[key] = new_value\\n            self._values.add(new_value)\\n        return self._map[key]\\n\\n    def anonymize(self, collection: str, key: str | None) -> str | None:\\n        if key is None:\\n            return None\\n        return self[f\"{collection}:{key}\"]']\n",
            "len of class: 761\n",
            "6 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/tree_export.py\n",
            "Imports: ['from __future__ import annotations', 'import contextlib', 'import gzip', 'import hashlib', 'import json', 'import sys', 'import uuid', 'from collections import defaultdict', 'from typing import Iterable, Optional, TextIO', 'from fastapi.encoders import jsonable_encoder', 'from oasst_backend.models import Message', 'from oasst_backend.models.message_tree_state import State as TreeState', 'from oasst_shared.schemas.export import ExportMessageEvent, ExportMessageEventEmoji, ExportMessageEventRanking, ExportMessageEventRating, ExportMessageNode, ExportMessageTree, LabelValues']\n",
            "functions: ['def sha256_hash(key: str, seed: int) -> str:\\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()', 'def prepare_export_message_node(\\n    message: Message,\\n    labels: Optional[LabelValues] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[str, list[ExportMessageEvent]] | None = None,\\n) -> ExportMessageNode:\\n    message_id = str(message.id)\\n    parent_id = str(message.parent_id) if message.parent_id else None\\n    user_id = str(message.user_id) if message.user_id else None\\n    if anonymizer is not None:\\n        message_id = anonymizer.anonymize(\"message\", message_id)\\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\\n        user_id = anonymizer.anonymize(\"user\", user_id)\\n        if events is not None:\\n            for event_key, event_values in events.items():\\n                for event in event_values:\\n                    match event_key:\\n                        case \"emoji\":\\n                            event: ExportMessageEventEmoji = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"rating\":\\n                            event: ExportMessageEventRating = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"ranking\":\\n                            event: ExportMessageEventRanking = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                            event.ranked_message_ids = [\\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\\n                            ]\\n                            if event.ranking_parent_id is not None:\\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\\n                            if event.message_tree_id is not None:\\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\\n                        case _:\\n                            raise ValueError(f\"Unknown event type {event_key}\")\\n    assert message_id is not None\\n    return ExportMessageNode(\\n        message_id=message_id,\\n        parent_id=parent_id,\\n        user_id=user_id,\\n        text=str(message.payload.payload.text),\\n        role=message.role,\\n        lang=message.lang,\\n        deleted=message.deleted,\\n        review_count=message.review_count,\\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\\n        synthetic=message.synthetic,\\n        model_name=message.model_name,\\n        emojis=message.emojis,\\n        rank=message.rank,\\n        labels=labels,\\n        events=events,\\n    )', 'def build_export_tree(\\n    message_tree_id: uuid.UUID,\\n    message_tree_state: TreeState,\\n    messages: list[Message],\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> ExportMessageTree:\\n    export_messages = [\\n        prepare_export_message_node(\\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n        )\\n        for m in messages\\n    ]\\n\\n    messages_by_parent = defaultdict(list)\\n    for message in export_messages:\\n        messages_by_parent[message.parent_id].append(message)\\n\\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\\n        node.replies = messages_by_parent[node.message_id]\\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\\n        for child in node.replies:\\n            assign_replies(child)\\n        return node\\n\\n    prompt = assign_replies(messages_by_parent[None][0])\\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)', 'def smart_open(filename: str = None) -> TextIO:\\n    if filename and filename != \"-\":\\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        fh = sys.stdout\\n\\n    try:\\n        yield fh\\n    finally:\\n        if fh is not sys.stdout:\\n            fh.close()', 'def write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for tree in trees:\\n            file_data = jsonable_encoder(tree, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")', 'def write_messages_to_file(\\n    filename: str | None,\\n    messages: Iterable[Message],\\n    use_compression: bool = True,\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for m in messages:\\n            export_message = prepare_export_message_node(\\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n            )\\n\\n            file_data = jsonable_encoder(export_message, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")']\n",
            "Classes: ['class Anonymizer:\\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\\n        self._map = {}\\n        self._values = set()\\n        self._seed = seed\\n        self._gen = value_generator\\n\\n    def __getitem__(self, key):\\n        if key not in self._map:\\n            new_value = self._gen(key, self._seed)\\n            if new_value in self._values:\\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\\n            self._map[key] = new_value\\n            self._values.add(new_value)\\n        return self._map[key]\\n\\n    def anonymize(self, collection: str, key: str | None) -> str | None:\\n        if key is None:\\n            return None\\n        return self[f\"{collection}:{key}\"]']\n",
            "len of class: 761\n",
            "6 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/tree_export.py\n",
            "Imports: ['from __future__ import annotations', 'import contextlib', 'import gzip', 'import hashlib', 'import json', 'import sys', 'import uuid', 'from collections import defaultdict', 'from typing import Iterable, Optional, TextIO', 'from fastapi.encoders import jsonable_encoder', 'from oasst_backend.models import Message', 'from oasst_backend.models.message_tree_state import State as TreeState', 'from oasst_shared.schemas.export import ExportMessageEvent, ExportMessageEventEmoji, ExportMessageEventRanking, ExportMessageEventRating, ExportMessageNode, ExportMessageTree, LabelValues']\n",
            "functions: ['def sha256_hash(key: str, seed: int) -> str:\\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()', 'def prepare_export_message_node(\\n    message: Message,\\n    labels: Optional[LabelValues] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[str, list[ExportMessageEvent]] | None = None,\\n) -> ExportMessageNode:\\n    message_id = str(message.id)\\n    parent_id = str(message.parent_id) if message.parent_id else None\\n    user_id = str(message.user_id) if message.user_id else None\\n    if anonymizer is not None:\\n        message_id = anonymizer.anonymize(\"message\", message_id)\\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\\n        user_id = anonymizer.anonymize(\"user\", user_id)\\n        if events is not None:\\n            for event_key, event_values in events.items():\\n                for event in event_values:\\n                    match event_key:\\n                        case \"emoji\":\\n                            event: ExportMessageEventEmoji = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"rating\":\\n                            event: ExportMessageEventRating = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"ranking\":\\n                            event: ExportMessageEventRanking = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                            event.ranked_message_ids = [\\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\\n                            ]\\n                            if event.ranking_parent_id is not None:\\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\\n                            if event.message_tree_id is not None:\\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\\n                        case _:\\n                            raise ValueError(f\"Unknown event type {event_key}\")\\n    assert message_id is not None\\n    return ExportMessageNode(\\n        message_id=message_id,\\n        parent_id=parent_id,\\n        user_id=user_id,\\n        text=str(message.payload.payload.text),\\n        role=message.role,\\n        lang=message.lang,\\n        deleted=message.deleted,\\n        review_count=message.review_count,\\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\\n        synthetic=message.synthetic,\\n        model_name=message.model_name,\\n        emojis=message.emojis,\\n        rank=message.rank,\\n        labels=labels,\\n        events=events,\\n    )', 'def build_export_tree(\\n    message_tree_id: uuid.UUID,\\n    message_tree_state: TreeState,\\n    messages: list[Message],\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> ExportMessageTree:\\n    export_messages = [\\n        prepare_export_message_node(\\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n        )\\n        for m in messages\\n    ]\\n\\n    messages_by_parent = defaultdict(list)\\n    for message in export_messages:\\n        messages_by_parent[message.parent_id].append(message)\\n\\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\\n        node.replies = messages_by_parent[node.message_id]\\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\\n        for child in node.replies:\\n            assign_replies(child)\\n        return node\\n\\n    prompt = assign_replies(messages_by_parent[None][0])\\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)', 'def smart_open(filename: str = None) -> TextIO:\\n    if filename and filename != \"-\":\\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        fh = sys.stdout\\n\\n    try:\\n        yield fh\\n    finally:\\n        if fh is not sys.stdout:\\n            fh.close()', 'def write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for tree in trees:\\n            file_data = jsonable_encoder(tree, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")', 'def write_messages_to_file(\\n    filename: str | None,\\n    messages: Iterable[Message],\\n    use_compression: bool = True,\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for m in messages:\\n            export_message = prepare_export_message_node(\\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n            )\\n\\n            file_data = jsonable_encoder(export_message, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")']\n",
            "Classes: ['class Anonymizer:\\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\\n        self._map = {}\\n        self._values = set()\\n        self._seed = seed\\n        self._gen = value_generator\\n\\n    def __getitem__(self, key):\\n        if key not in self._map:\\n            new_value = self._gen(key, self._seed)\\n            if new_value in self._values:\\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\\n            self._map[key] = new_value\\n            self._values.add(new_value)\\n        return self._map[key]\\n\\n    def anonymize(self, collection: str, key: str | None) -> str | None:\\n        if key is None:\\n            return None\\n        return self[f\"{collection}:{key}\"]']\n",
            "len of class: 761\n",
            "6 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/tree_export.py\n",
            "Imports: ['from __future__ import annotations', 'import contextlib', 'import gzip', 'import hashlib', 'import json', 'import sys', 'import uuid', 'from collections import defaultdict', 'from typing import Iterable, Optional, TextIO', 'from fastapi.encoders import jsonable_encoder', 'from oasst_backend.models import Message', 'from oasst_backend.models.message_tree_state import State as TreeState', 'from oasst_shared.schemas.export import ExportMessageEvent, ExportMessageEventEmoji, ExportMessageEventRanking, ExportMessageEventRating, ExportMessageNode, ExportMessageTree, LabelValues']\n",
            "functions: ['def sha256_hash(key: str, seed: int) -> str:\\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()', 'def prepare_export_message_node(\\n    message: Message,\\n    labels: Optional[LabelValues] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[str, list[ExportMessageEvent]] | None = None,\\n) -> ExportMessageNode:\\n    message_id = str(message.id)\\n    parent_id = str(message.parent_id) if message.parent_id else None\\n    user_id = str(message.user_id) if message.user_id else None\\n    if anonymizer is not None:\\n        message_id = anonymizer.anonymize(\"message\", message_id)\\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\\n        user_id = anonymizer.anonymize(\"user\", user_id)\\n        if events is not None:\\n            for event_key, event_values in events.items():\\n                for event in event_values:\\n                    match event_key:\\n                        case \"emoji\":\\n                            event: ExportMessageEventEmoji = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"rating\":\\n                            event: ExportMessageEventRating = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                        case \"ranking\":\\n                            event: ExportMessageEventRanking = event\\n                            if event.user_id is not None:\\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\\n                            event.ranked_message_ids = [\\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\\n                            ]\\n                            if event.ranking_parent_id is not None:\\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\\n                            if event.message_tree_id is not None:\\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\\n                        case _:\\n                            raise ValueError(f\"Unknown event type {event_key}\")\\n    assert message_id is not None\\n    return ExportMessageNode(\\n        message_id=message_id,\\n        parent_id=parent_id,\\n        user_id=user_id,\\n        text=str(message.payload.payload.text),\\n        role=message.role,\\n        lang=message.lang,\\n        deleted=message.deleted,\\n        review_count=message.review_count,\\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\\n        synthetic=message.synthetic,\\n        model_name=message.model_name,\\n        emojis=message.emojis,\\n        rank=message.rank,\\n        labels=labels,\\n        events=events,\\n    )', 'def build_export_tree(\\n    message_tree_id: uuid.UUID,\\n    message_tree_state: TreeState,\\n    messages: list[Message],\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> ExportMessageTree:\\n    export_messages = [\\n        prepare_export_message_node(\\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n        )\\n        for m in messages\\n    ]\\n\\n    messages_by_parent = defaultdict(list)\\n    for message in export_messages:\\n        messages_by_parent[message.parent_id].append(message)\\n\\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\\n        node.replies = messages_by_parent[node.message_id]\\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\\n        for child in node.replies:\\n            assign_replies(child)\\n        return node\\n\\n    prompt = assign_replies(messages_by_parent[None][0])\\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)', 'def smart_open(filename: str = None) -> TextIO:\\n    if filename and filename != \"-\":\\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        fh = sys.stdout\\n\\n    try:\\n        yield fh\\n    finally:\\n        if fh is not sys.stdout:\\n            fh.close()', 'def write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for tree in trees:\\n            file_data = jsonable_encoder(tree, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")', 'def write_messages_to_file(\\n    filename: str | None,\\n    messages: Iterable[Message],\\n    use_compression: bool = True,\\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\\n    anonymizer: Anonymizer | None = None,\\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\\n) -> None:\\n    out_buff: TextIO\\n\\n    if use_compression:\\n        if not filename:\\n            raise RuntimeError(\"File name must be specified when using compression.\")\\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\\n    else:\\n        out_buff = smart_open(filename)\\n\\n    with out_buff as f:\\n        for m in messages:\\n            export_message = prepare_export_message_node(\\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\\n            )\\n\\n            file_data = jsonable_encoder(export_message, exclude_none=True)\\n            json.dump(file_data, f)\\n            f.write(\"\\\\n\")']\n",
            "Classes: ['class Anonymizer:\\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\\n        self._map = {}\\n        self._values = set()\\n        self._seed = seed\\n        self._gen = value_generator\\n\\n    def __getitem__(self, key):\\n        if key not in self._map:\\n            new_value = self._gen(key, self._seed)\\n            if new_value in self._values:\\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\\n            self._map[key] = new_value\\n            self._values.add(new_value)\\n        return self._map[key]\\n\\n    def anonymize(self, collection: str, key: str | None) -> str | None:\\n        if key is None:\\n            return None\\n        return self[f\"{collection}:{key}\"]']\n",
            "len of class: 761\n",
            "6 1\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/similarity_functions.py\n",
            "Imports: ['import math', 'import numpy as np', 'import scipy.sparse as sp', 'import torch', 'import torch.nn.functional as F', 'from pandas import DataFrame', 'from sentence_transformers import SentenceTransformer', 'from torch import Tensor', 'from tqdm import tqdm']\n",
            "functions: ['def embed_data(\\n    data: DataFrame,\\n    key: str = \"query\",\\n    model_name: str = \"all-MiniLM-L6-v2\",\\n    cores: int = 1,\\n    gpu: bool = False,\\n    batch_size: int = 128,\\n):\\n    \"\"\"\\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\\n    \"\"\"\\n    print(\"Embedding data\")\\n    model = SentenceTransformer(model_name)\\n    print(\"Model loaded\")\\n\\n    sentences = data[key].tolist()\\n    unique_sentences = data[key].unique()\\n    print(\"Unique sentences\", len(unique_sentences))\\n\\n    if cores == 1:\\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\\n    else:\\n        devices = [\"cpu\"] * cores\\n        if gpu:\\n            devices = None  # use all CUDA devices\\n\\n        # Start the multi-process pool on multiple devices\\n        print(\"Multi-process pool starting\")\\n        pool = model.start_multi_process_pool(devices)\\n        print(\"Multi-process pool started\")\\n\\n        chunk_size = math.ceil(len(unique_sentences) / cores)\\n\\n        # Compute the embeddings using the multi-process pool\\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\\n        model.stop_multi_process_pool(pool)\\n\\n    print(\"Embeddings computed\")\\n\\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\\n\\n    return embeddings', 'def cos_sim(a: Tensor, b: Tensor):\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(a, torch.Tensor):\\n        a = torch.tensor(np.array(a))\\n\\n    if not isinstance(b, torch.Tensor):\\n        b = torch.tensor(np.array(b))\\n\\n    if len(a.shape) == 1:\\n        a = a.unsqueeze(0)\\n\\n    if len(b.shape) == 1:\\n        b = b.unsqueeze(0)\\n\\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\\n    return torch.mm(a_norm, b_norm.transpose(0, 1))', 'def cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    Using torch.nn.functional.cosine_similarity\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(np.array(embs_a))\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(np.array(embs_b))\\n\\n    if len(embs_a.shape) == 1:\\n        embs_a = embs_a.unsqueeze(0)\\n\\n    if len(embs_b.shape) == 1:\\n        embs_b = embs_b.unsqueeze(0)\\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\\n    return A', 'def gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\\n    \"\"\"\\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\\n    :param sigma: Width of the Gaussian kernel.\\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(embs_a)\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(embs_b)\\n\\n    # Compute the pairwise distances between the embeddings\\n    dist_matrix = torch.cdist(embs_a, embs_b)\\n\\n    # Compute the Gaussian kernel matrix\\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\\n\\n    return kernel_matrix', 'def compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\\n    # match case to kernel type\\n    if kernel_type == \"gaussian\":\\n        A = gaussian_kernel_torch(embs, embs)\\n    if kernel_type == \"cosine\":\\n        A = cos_sim_torch(embs, embs)\\n    adj_matrix = torch.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\\n    return adj_matrix', 'def k_hop_message_passing(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array): The adjacency matrix of the graph.\\n    node_features (numpy array): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features', 'def k_hop_message_passing_sparse(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    # Convert input matrices to sparse matrices if they are not already\\n    if not sp.issparse(A):\\n        A = sp.csr_matrix(A)\\n    if not sp.issparse(node_features):\\n        node_features = sp.csr_matrix(node_features)\\n\\n    # Compute the k-hop adjacency matrix and the aggregated features\\n    A_k = A.copy()\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        # Compute the message passing for the k-hop neighborhood\\n        message = A_k.dot(node_features)\\n        # Apply a GCN layer to aggregate the messages\\n        agg_features = A_k.dot(agg_features) + message\\n        # Update the k-hop adjacency matrix by adding new edges\\n        A_k += A_k.dot(A)\\n\\n    return A_k.toarray(), agg_features.toarray()']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/similarity_functions.py\n",
            "Imports: ['import math', 'import numpy as np', 'import scipy.sparse as sp', 'import torch', 'import torch.nn.functional as F', 'from pandas import DataFrame', 'from sentence_transformers import SentenceTransformer', 'from torch import Tensor', 'from tqdm import tqdm']\n",
            "functions: ['def embed_data(\\n    data: DataFrame,\\n    key: str = \"query\",\\n    model_name: str = \"all-MiniLM-L6-v2\",\\n    cores: int = 1,\\n    gpu: bool = False,\\n    batch_size: int = 128,\\n):\\n    \"\"\"\\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\\n    \"\"\"\\n    print(\"Embedding data\")\\n    model = SentenceTransformer(model_name)\\n    print(\"Model loaded\")\\n\\n    sentences = data[key].tolist()\\n    unique_sentences = data[key].unique()\\n    print(\"Unique sentences\", len(unique_sentences))\\n\\n    if cores == 1:\\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\\n    else:\\n        devices = [\"cpu\"] * cores\\n        if gpu:\\n            devices = None  # use all CUDA devices\\n\\n        # Start the multi-process pool on multiple devices\\n        print(\"Multi-process pool starting\")\\n        pool = model.start_multi_process_pool(devices)\\n        print(\"Multi-process pool started\")\\n\\n        chunk_size = math.ceil(len(unique_sentences) / cores)\\n\\n        # Compute the embeddings using the multi-process pool\\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\\n        model.stop_multi_process_pool(pool)\\n\\n    print(\"Embeddings computed\")\\n\\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\\n\\n    return embeddings', 'def cos_sim(a: Tensor, b: Tensor):\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(a, torch.Tensor):\\n        a = torch.tensor(np.array(a))\\n\\n    if not isinstance(b, torch.Tensor):\\n        b = torch.tensor(np.array(b))\\n\\n    if len(a.shape) == 1:\\n        a = a.unsqueeze(0)\\n\\n    if len(b.shape) == 1:\\n        b = b.unsqueeze(0)\\n\\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\\n    return torch.mm(a_norm, b_norm.transpose(0, 1))', 'def cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    Using torch.nn.functional.cosine_similarity\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(np.array(embs_a))\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(np.array(embs_b))\\n\\n    if len(embs_a.shape) == 1:\\n        embs_a = embs_a.unsqueeze(0)\\n\\n    if len(embs_b.shape) == 1:\\n        embs_b = embs_b.unsqueeze(0)\\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\\n    return A', 'def gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\\n    \"\"\"\\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\\n    :param sigma: Width of the Gaussian kernel.\\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(embs_a)\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(embs_b)\\n\\n    # Compute the pairwise distances between the embeddings\\n    dist_matrix = torch.cdist(embs_a, embs_b)\\n\\n    # Compute the Gaussian kernel matrix\\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\\n\\n    return kernel_matrix', 'def compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\\n    # match case to kernel type\\n    if kernel_type == \"gaussian\":\\n        A = gaussian_kernel_torch(embs, embs)\\n    if kernel_type == \"cosine\":\\n        A = cos_sim_torch(embs, embs)\\n    adj_matrix = torch.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\\n    return adj_matrix', 'def k_hop_message_passing(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array): The adjacency matrix of the graph.\\n    node_features (numpy array): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features', 'def k_hop_message_passing_sparse(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    # Convert input matrices to sparse matrices if they are not already\\n    if not sp.issparse(A):\\n        A = sp.csr_matrix(A)\\n    if not sp.issparse(node_features):\\n        node_features = sp.csr_matrix(node_features)\\n\\n    # Compute the k-hop adjacency matrix and the aggregated features\\n    A_k = A.copy()\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        # Compute the message passing for the k-hop neighborhood\\n        message = A_k.dot(node_features)\\n        # Apply a GCN layer to aggregate the messages\\n        agg_features = A_k.dot(agg_features) + message\\n        # Update the k-hop adjacency matrix by adding new edges\\n        A_k += A_k.dot(A)\\n\\n    return A_k.toarray(), agg_features.toarray()']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/similarity_functions.py\n",
            "Imports: ['import math', 'import numpy as np', 'import scipy.sparse as sp', 'import torch', 'import torch.nn.functional as F', 'from pandas import DataFrame', 'from sentence_transformers import SentenceTransformer', 'from torch import Tensor', 'from tqdm import tqdm']\n",
            "functions: ['def embed_data(\\n    data: DataFrame,\\n    key: str = \"query\",\\n    model_name: str = \"all-MiniLM-L6-v2\",\\n    cores: int = 1,\\n    gpu: bool = False,\\n    batch_size: int = 128,\\n):\\n    \"\"\"\\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\\n    \"\"\"\\n    print(\"Embedding data\")\\n    model = SentenceTransformer(model_name)\\n    print(\"Model loaded\")\\n\\n    sentences = data[key].tolist()\\n    unique_sentences = data[key].unique()\\n    print(\"Unique sentences\", len(unique_sentences))\\n\\n    if cores == 1:\\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\\n    else:\\n        devices = [\"cpu\"] * cores\\n        if gpu:\\n            devices = None  # use all CUDA devices\\n\\n        # Start the multi-process pool on multiple devices\\n        print(\"Multi-process pool starting\")\\n        pool = model.start_multi_process_pool(devices)\\n        print(\"Multi-process pool started\")\\n\\n        chunk_size = math.ceil(len(unique_sentences) / cores)\\n\\n        # Compute the embeddings using the multi-process pool\\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\\n        model.stop_multi_process_pool(pool)\\n\\n    print(\"Embeddings computed\")\\n\\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\\n\\n    return embeddings', 'def cos_sim(a: Tensor, b: Tensor):\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(a, torch.Tensor):\\n        a = torch.tensor(np.array(a))\\n\\n    if not isinstance(b, torch.Tensor):\\n        b = torch.tensor(np.array(b))\\n\\n    if len(a.shape) == 1:\\n        a = a.unsqueeze(0)\\n\\n    if len(b.shape) == 1:\\n        b = b.unsqueeze(0)\\n\\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\\n    return torch.mm(a_norm, b_norm.transpose(0, 1))', 'def cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    Using torch.nn.functional.cosine_similarity\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(np.array(embs_a))\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(np.array(embs_b))\\n\\n    if len(embs_a.shape) == 1:\\n        embs_a = embs_a.unsqueeze(0)\\n\\n    if len(embs_b.shape) == 1:\\n        embs_b = embs_b.unsqueeze(0)\\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\\n    return A', 'def gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\\n    \"\"\"\\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\\n    :param sigma: Width of the Gaussian kernel.\\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(embs_a)\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(embs_b)\\n\\n    # Compute the pairwise distances between the embeddings\\n    dist_matrix = torch.cdist(embs_a, embs_b)\\n\\n    # Compute the Gaussian kernel matrix\\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\\n\\n    return kernel_matrix', 'def compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\\n    # match case to kernel type\\n    if kernel_type == \"gaussian\":\\n        A = gaussian_kernel_torch(embs, embs)\\n    if kernel_type == \"cosine\":\\n        A = cos_sim_torch(embs, embs)\\n    adj_matrix = torch.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\\n    return adj_matrix', 'def k_hop_message_passing(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array): The adjacency matrix of the graph.\\n    node_features (numpy array): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features', 'def k_hop_message_passing_sparse(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    # Convert input matrices to sparse matrices if they are not already\\n    if not sp.issparse(A):\\n        A = sp.csr_matrix(A)\\n    if not sp.issparse(node_features):\\n        node_features = sp.csr_matrix(node_features)\\n\\n    # Compute the k-hop adjacency matrix and the aggregated features\\n    A_k = A.copy()\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        # Compute the message passing for the k-hop neighborhood\\n        message = A_k.dot(node_features)\\n        # Apply a GCN layer to aggregate the messages\\n        agg_features = A_k.dot(agg_features) + message\\n        # Update the k-hop adjacency matrix by adding new edges\\n        A_k += A_k.dot(A)\\n\\n    return A_k.toarray(), agg_features.toarray()']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/similarity_functions.py\n",
            "Imports: ['import math', 'import numpy as np', 'import scipy.sparse as sp', 'import torch', 'import torch.nn.functional as F', 'from pandas import DataFrame', 'from sentence_transformers import SentenceTransformer', 'from torch import Tensor', 'from tqdm import tqdm']\n",
            "functions: ['def embed_data(\\n    data: DataFrame,\\n    key: str = \"query\",\\n    model_name: str = \"all-MiniLM-L6-v2\",\\n    cores: int = 1,\\n    gpu: bool = False,\\n    batch_size: int = 128,\\n):\\n    \"\"\"\\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\\n    \"\"\"\\n    print(\"Embedding data\")\\n    model = SentenceTransformer(model_name)\\n    print(\"Model loaded\")\\n\\n    sentences = data[key].tolist()\\n    unique_sentences = data[key].unique()\\n    print(\"Unique sentences\", len(unique_sentences))\\n\\n    if cores == 1:\\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\\n    else:\\n        devices = [\"cpu\"] * cores\\n        if gpu:\\n            devices = None  # use all CUDA devices\\n\\n        # Start the multi-process pool on multiple devices\\n        print(\"Multi-process pool starting\")\\n        pool = model.start_multi_process_pool(devices)\\n        print(\"Multi-process pool started\")\\n\\n        chunk_size = math.ceil(len(unique_sentences) / cores)\\n\\n        # Compute the embeddings using the multi-process pool\\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\\n        model.stop_multi_process_pool(pool)\\n\\n    print(\"Embeddings computed\")\\n\\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\\n\\n    return embeddings', 'def cos_sim(a: Tensor, b: Tensor):\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(a, torch.Tensor):\\n        a = torch.tensor(np.array(a))\\n\\n    if not isinstance(b, torch.Tensor):\\n        b = torch.tensor(np.array(b))\\n\\n    if len(a.shape) == 1:\\n        a = a.unsqueeze(0)\\n\\n    if len(b.shape) == 1:\\n        b = b.unsqueeze(0)\\n\\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\\n    return torch.mm(a_norm, b_norm.transpose(0, 1))', 'def cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    Using torch.nn.functional.cosine_similarity\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(np.array(embs_a))\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(np.array(embs_b))\\n\\n    if len(embs_a.shape) == 1:\\n        embs_a = embs_a.unsqueeze(0)\\n\\n    if len(embs_b.shape) == 1:\\n        embs_b = embs_b.unsqueeze(0)\\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\\n    return A', 'def gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\\n    \"\"\"\\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\\n    :param sigma: Width of the Gaussian kernel.\\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(embs_a)\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(embs_b)\\n\\n    # Compute the pairwise distances between the embeddings\\n    dist_matrix = torch.cdist(embs_a, embs_b)\\n\\n    # Compute the Gaussian kernel matrix\\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\\n\\n    return kernel_matrix', 'def compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\\n    # match case to kernel type\\n    if kernel_type == \"gaussian\":\\n        A = gaussian_kernel_torch(embs, embs)\\n    if kernel_type == \"cosine\":\\n        A = cos_sim_torch(embs, embs)\\n    adj_matrix = torch.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\\n    return adj_matrix', 'def k_hop_message_passing(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array): The adjacency matrix of the graph.\\n    node_features (numpy array): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features', 'def k_hop_message_passing_sparse(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    # Convert input matrices to sparse matrices if they are not already\\n    if not sp.issparse(A):\\n        A = sp.csr_matrix(A)\\n    if not sp.issparse(node_features):\\n        node_features = sp.csr_matrix(node_features)\\n\\n    # Compute the k-hop adjacency matrix and the aggregated features\\n    A_k = A.copy()\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        # Compute the message passing for the k-hop neighborhood\\n        message = A_k.dot(node_features)\\n        # Apply a GCN layer to aggregate the messages\\n        agg_features = A_k.dot(agg_features) + message\\n        # Update the k-hop adjacency matrix by adding new edges\\n        A_k += A_k.dot(A)\\n\\n    return A_k.toarray(), agg_features.toarray()']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/similarity_functions.py\n",
            "Imports: ['import math', 'import numpy as np', 'import scipy.sparse as sp', 'import torch', 'import torch.nn.functional as F', 'from pandas import DataFrame', 'from sentence_transformers import SentenceTransformer', 'from torch import Tensor', 'from tqdm import tqdm']\n",
            "functions: ['def embed_data(\\n    data: DataFrame,\\n    key: str = \"query\",\\n    model_name: str = \"all-MiniLM-L6-v2\",\\n    cores: int = 1,\\n    gpu: bool = False,\\n    batch_size: int = 128,\\n):\\n    \"\"\"\\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\\n    \"\"\"\\n    print(\"Embedding data\")\\n    model = SentenceTransformer(model_name)\\n    print(\"Model loaded\")\\n\\n    sentences = data[key].tolist()\\n    unique_sentences = data[key].unique()\\n    print(\"Unique sentences\", len(unique_sentences))\\n\\n    if cores == 1:\\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\\n    else:\\n        devices = [\"cpu\"] * cores\\n        if gpu:\\n            devices = None  # use all CUDA devices\\n\\n        # Start the multi-process pool on multiple devices\\n        print(\"Multi-process pool starting\")\\n        pool = model.start_multi_process_pool(devices)\\n        print(\"Multi-process pool started\")\\n\\n        chunk_size = math.ceil(len(unique_sentences) / cores)\\n\\n        # Compute the embeddings using the multi-process pool\\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\\n        model.stop_multi_process_pool(pool)\\n\\n    print(\"Embeddings computed\")\\n\\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\\n\\n    return embeddings', 'def cos_sim(a: Tensor, b: Tensor):\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(a, torch.Tensor):\\n        a = torch.tensor(np.array(a))\\n\\n    if not isinstance(b, torch.Tensor):\\n        b = torch.tensor(np.array(b))\\n\\n    if len(a.shape) == 1:\\n        a = a.unsqueeze(0)\\n\\n    if len(b.shape) == 1:\\n        b = b.unsqueeze(0)\\n\\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\\n    return torch.mm(a_norm, b_norm.transpose(0, 1))', 'def cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    Using torch.nn.functional.cosine_similarity\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(np.array(embs_a))\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(np.array(embs_b))\\n\\n    if len(embs_a.shape) == 1:\\n        embs_a = embs_a.unsqueeze(0)\\n\\n    if len(embs_b.shape) == 1:\\n        embs_b = embs_b.unsqueeze(0)\\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\\n    return A', 'def gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\\n    \"\"\"\\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\\n    :param sigma: Width of the Gaussian kernel.\\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(embs_a)\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(embs_b)\\n\\n    # Compute the pairwise distances between the embeddings\\n    dist_matrix = torch.cdist(embs_a, embs_b)\\n\\n    # Compute the Gaussian kernel matrix\\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\\n\\n    return kernel_matrix', 'def compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\\n    # match case to kernel type\\n    if kernel_type == \"gaussian\":\\n        A = gaussian_kernel_torch(embs, embs)\\n    if kernel_type == \"cosine\":\\n        A = cos_sim_torch(embs, embs)\\n    adj_matrix = torch.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\\n    return adj_matrix', 'def k_hop_message_passing(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array): The adjacency matrix of the graph.\\n    node_features (numpy array): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features', 'def k_hop_message_passing_sparse(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    # Convert input matrices to sparse matrices if they are not already\\n    if not sp.issparse(A):\\n        A = sp.csr_matrix(A)\\n    if not sp.issparse(node_features):\\n        node_features = sp.csr_matrix(node_features)\\n\\n    # Compute the k-hop adjacency matrix and the aggregated features\\n    A_k = A.copy()\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        # Compute the message passing for the k-hop neighborhood\\n        message = A_k.dot(node_features)\\n        # Apply a GCN layer to aggregate the messages\\n        agg_features = A_k.dot(agg_features) + message\\n        # Update the k-hop adjacency matrix by adding new edges\\n        A_k += A_k.dot(A)\\n\\n    return A_k.toarray(), agg_features.toarray()']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/similarity_functions.py\n",
            "Imports: ['import math', 'import numpy as np', 'import scipy.sparse as sp', 'import torch', 'import torch.nn.functional as F', 'from pandas import DataFrame', 'from sentence_transformers import SentenceTransformer', 'from torch import Tensor', 'from tqdm import tqdm']\n",
            "functions: ['def embed_data(\\n    data: DataFrame,\\n    key: str = \"query\",\\n    model_name: str = \"all-MiniLM-L6-v2\",\\n    cores: int = 1,\\n    gpu: bool = False,\\n    batch_size: int = 128,\\n):\\n    \"\"\"\\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\\n    \"\"\"\\n    print(\"Embedding data\")\\n    model = SentenceTransformer(model_name)\\n    print(\"Model loaded\")\\n\\n    sentences = data[key].tolist()\\n    unique_sentences = data[key].unique()\\n    print(\"Unique sentences\", len(unique_sentences))\\n\\n    if cores == 1:\\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\\n    else:\\n        devices = [\"cpu\"] * cores\\n        if gpu:\\n            devices = None  # use all CUDA devices\\n\\n        # Start the multi-process pool on multiple devices\\n        print(\"Multi-process pool starting\")\\n        pool = model.start_multi_process_pool(devices)\\n        print(\"Multi-process pool started\")\\n\\n        chunk_size = math.ceil(len(unique_sentences) / cores)\\n\\n        # Compute the embeddings using the multi-process pool\\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\\n        model.stop_multi_process_pool(pool)\\n\\n    print(\"Embeddings computed\")\\n\\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\\n\\n    return embeddings', 'def cos_sim(a: Tensor, b: Tensor):\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(a, torch.Tensor):\\n        a = torch.tensor(np.array(a))\\n\\n    if not isinstance(b, torch.Tensor):\\n        b = torch.tensor(np.array(b))\\n\\n    if len(a.shape) == 1:\\n        a = a.unsqueeze(0)\\n\\n    if len(b.shape) == 1:\\n        b = b.unsqueeze(0)\\n\\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\\n    return torch.mm(a_norm, b_norm.transpose(0, 1))', 'def cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\\n    \"\"\"\\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\\n    Using torch.nn.functional.cosine_similarity\\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(np.array(embs_a))\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(np.array(embs_b))\\n\\n    if len(embs_a.shape) == 1:\\n        embs_a = embs_a.unsqueeze(0)\\n\\n    if len(embs_b.shape) == 1:\\n        embs_b = embs_b.unsqueeze(0)\\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\\n    return A', 'def gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\\n    \"\"\"\\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\\n    :param sigma: Width of the Gaussian kernel.\\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\\n    \"\"\"\\n    if not isinstance(embs_a, torch.Tensor):\\n        embs_a = torch.tensor(embs_a)\\n\\n    if not isinstance(embs_b, torch.Tensor):\\n        embs_b = torch.tensor(embs_b)\\n\\n    # Compute the pairwise distances between the embeddings\\n    dist_matrix = torch.cdist(embs_a, embs_b)\\n\\n    # Compute the Gaussian kernel matrix\\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\\n\\n    return kernel_matrix', 'def compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\\n    # match case to kernel type\\n    if kernel_type == \"gaussian\":\\n        A = gaussian_kernel_torch(embs, embs)\\n    if kernel_type == \"cosine\":\\n        A = cos_sim_torch(embs, embs)\\n    adj_matrix = torch.zeros_like(A)\\n    adj_matrix[A > threshold] = 1\\n    adj_matrix[A <= threshold] = 0\\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\\n    return adj_matrix', 'def k_hop_message_passing(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array): The adjacency matrix of the graph.\\n    node_features (numpy array): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    print(\"Compute the k-hop adjacency matrix\")\\n    A_k = np.linalg.matrix_power(A, k)\\n\\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\\n\\n    return A_k, agg_features', 'def k_hop_message_passing_sparse(A, node_features, k):\\n    \"\"\"\\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\\n\\n    Parameters:\\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\\n    k (int): The number of hops for message passing.\\n\\n    Returns:\\n    A_k (numpy array): The k-hop adjacency matrix.\\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\\n    \"\"\"\\n\\n    # Convert input matrices to sparse matrices if they are not already\\n    if not sp.issparse(A):\\n        A = sp.csr_matrix(A)\\n    if not sp.issparse(node_features):\\n        node_features = sp.csr_matrix(node_features)\\n\\n    # Compute the k-hop adjacency matrix and the aggregated features\\n    A_k = A.copy()\\n    agg_features = node_features.copy()\\n\\n    for i in tqdm(range(k)):\\n        # Compute the message passing for the k-hop neighborhood\\n        message = A_k.dot(node_features)\\n        # Apply a GCN layer to aggregate the messages\\n        agg_features = A_k.dot(agg_features) + message\\n        # Update the k-hop adjacency matrix by adding new edges\\n        A_k += A_k.dot(A)\\n\\n    return A_k.toarray(), agg_features.toarray()']\n",
            "Classes: []\n",
            "7 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/message_tree_topic_modeling.py\n",
            "Imports: ['import argparse', 'from bertopic import BERTopic', 'from bertopic.representation import MaximalMarginalRelevance', 'from bertopic.vectorizers import ClassTfidfTransformer', 'from exported_tree_loading import load_data', 'from sentence_transformers import SentenceTransformer', 'from similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse', 'from sklearn.feature_extraction.text import CountVectorizer']\n",
            "functions: ['def argument_parsing():\\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\\n    parser.add_argument(\"--cores\", type=int, default=1)\\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\\n    parser.add_argument(\"--batch_size\", type=int, default=128)\\n    parser.add_argument(\"--k\", type=int, default=2)\\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\\n\\n    args = parser.parse_args()\\n    return args', 'def load_topic_model(args):\\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\\n    model = SentenceTransformer(MODEL_NAME)\\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\\n    topic_model = BERTopic(\\n        nr_topics=\"auto\",\\n        min_topic_size=args.min_topic_size,\\n        representation_model=representation_model,\\n        vectorizer_model=vectorizer_model,\\n        ctfidf_model=ctfidf_model,\\n        embedding_model=model,\\n    )\\n    return topic_model', 'def fit_topic_model(topic_model, data, embeddings, key=\"query\"):\\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\\n    return topics, probs', 'def get_topic_info(topic_model):\\n    return topic_model.get_topic_info()', 'def reduce_topics(topic_model, data, nr_topics, key=\"query\"):\\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\\n    return topic_model', 'def get_representative_docs(topic_model):\\n    return topic_model.get_representative_docs()', 'def reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\\n    if strategy == \"c-tf-idf\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\\n    elif strategy == \"embeddings\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\\n    elif strategy == \"distributions\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\\n    else:\\n        raise ValueError(\"Invalid strategy\")\\n    return new_topics', 'def compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\\n    tree = topic_model.get_topic_tree(hierarchical_topics)\\n    return hierarchical_topics, tree']\n",
            "Classes: []\n",
            "8 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/message_tree_topic_modeling.py\n",
            "Imports: ['import argparse', 'from bertopic import BERTopic', 'from bertopic.representation import MaximalMarginalRelevance', 'from bertopic.vectorizers import ClassTfidfTransformer', 'from exported_tree_loading import load_data', 'from sentence_transformers import SentenceTransformer', 'from similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse', 'from sklearn.feature_extraction.text import CountVectorizer']\n",
            "functions: ['def argument_parsing():\\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\\n    parser.add_argument(\"--cores\", type=int, default=1)\\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\\n    parser.add_argument(\"--batch_size\", type=int, default=128)\\n    parser.add_argument(\"--k\", type=int, default=2)\\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\\n\\n    args = parser.parse_args()\\n    return args', 'def load_topic_model(args):\\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\\n    model = SentenceTransformer(MODEL_NAME)\\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\\n    topic_model = BERTopic(\\n        nr_topics=\"auto\",\\n        min_topic_size=args.min_topic_size,\\n        representation_model=representation_model,\\n        vectorizer_model=vectorizer_model,\\n        ctfidf_model=ctfidf_model,\\n        embedding_model=model,\\n    )\\n    return topic_model', 'def fit_topic_model(topic_model, data, embeddings, key=\"query\"):\\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\\n    return topics, probs', 'def get_topic_info(topic_model):\\n    return topic_model.get_topic_info()', 'def reduce_topics(topic_model, data, nr_topics, key=\"query\"):\\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\\n    return topic_model', 'def get_representative_docs(topic_model):\\n    return topic_model.get_representative_docs()', 'def reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\\n    if strategy == \"c-tf-idf\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\\n    elif strategy == \"embeddings\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\\n    elif strategy == \"distributions\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\\n    else:\\n        raise ValueError(\"Invalid strategy\")\\n    return new_topics', 'def compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\\n    tree = topic_model.get_topic_tree(hierarchical_topics)\\n    return hierarchical_topics, tree']\n",
            "Classes: []\n",
            "8 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/message_tree_topic_modeling.py\n",
            "Imports: ['import argparse', 'from bertopic import BERTopic', 'from bertopic.representation import MaximalMarginalRelevance', 'from bertopic.vectorizers import ClassTfidfTransformer', 'from exported_tree_loading import load_data', 'from sentence_transformers import SentenceTransformer', 'from similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse', 'from sklearn.feature_extraction.text import CountVectorizer']\n",
            "functions: ['def argument_parsing():\\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\\n    parser.add_argument(\"--cores\", type=int, default=1)\\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\\n    parser.add_argument(\"--batch_size\", type=int, default=128)\\n    parser.add_argument(\"--k\", type=int, default=2)\\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\\n\\n    args = parser.parse_args()\\n    return args', 'def load_topic_model(args):\\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\\n    model = SentenceTransformer(MODEL_NAME)\\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\\n    topic_model = BERTopic(\\n        nr_topics=\"auto\",\\n        min_topic_size=args.min_topic_size,\\n        representation_model=representation_model,\\n        vectorizer_model=vectorizer_model,\\n        ctfidf_model=ctfidf_model,\\n        embedding_model=model,\\n    )\\n    return topic_model', 'def fit_topic_model(topic_model, data, embeddings, key=\"query\"):\\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\\n    return topics, probs', 'def get_topic_info(topic_model):\\n    return topic_model.get_topic_info()', 'def reduce_topics(topic_model, data, nr_topics, key=\"query\"):\\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\\n    return topic_model', 'def get_representative_docs(topic_model):\\n    return topic_model.get_representative_docs()', 'def reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\\n    if strategy == \"c-tf-idf\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\\n    elif strategy == \"embeddings\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\\n    elif strategy == \"distributions\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\\n    else:\\n        raise ValueError(\"Invalid strategy\")\\n    return new_topics', 'def compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\\n    tree = topic_model.get_topic_tree(hierarchical_topics)\\n    return hierarchical_topics, tree']\n",
            "Classes: []\n",
            "8 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/message_tree_topic_modeling.py\n",
            "Imports: ['import argparse', 'from bertopic import BERTopic', 'from bertopic.representation import MaximalMarginalRelevance', 'from bertopic.vectorizers import ClassTfidfTransformer', 'from exported_tree_loading import load_data', 'from sentence_transformers import SentenceTransformer', 'from similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse', 'from sklearn.feature_extraction.text import CountVectorizer']\n",
            "functions: ['def argument_parsing():\\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\\n    parser.add_argument(\"--cores\", type=int, default=1)\\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\\n    parser.add_argument(\"--batch_size\", type=int, default=128)\\n    parser.add_argument(\"--k\", type=int, default=2)\\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\\n\\n    args = parser.parse_args()\\n    return args', 'def load_topic_model(args):\\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\\n    model = SentenceTransformer(MODEL_NAME)\\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\\n    topic_model = BERTopic(\\n        nr_topics=\"auto\",\\n        min_topic_size=args.min_topic_size,\\n        representation_model=representation_model,\\n        vectorizer_model=vectorizer_model,\\n        ctfidf_model=ctfidf_model,\\n        embedding_model=model,\\n    )\\n    return topic_model', 'def fit_topic_model(topic_model, data, embeddings, key=\"query\"):\\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\\n    return topics, probs', 'def get_topic_info(topic_model):\\n    return topic_model.get_topic_info()', 'def reduce_topics(topic_model, data, nr_topics, key=\"query\"):\\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\\n    return topic_model', 'def get_representative_docs(topic_model):\\n    return topic_model.get_representative_docs()', 'def reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\\n    if strategy == \"c-tf-idf\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\\n    elif strategy == \"embeddings\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\\n    elif strategy == \"distributions\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\\n    else:\\n        raise ValueError(\"Invalid strategy\")\\n    return new_topics', 'def compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\\n    tree = topic_model.get_topic_tree(hierarchical_topics)\\n    return hierarchical_topics, tree']\n",
            "Classes: []\n",
            "8 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/message_tree_topic_modeling.py\n",
            "Imports: ['import argparse', 'from bertopic import BERTopic', 'from bertopic.representation import MaximalMarginalRelevance', 'from bertopic.vectorizers import ClassTfidfTransformer', 'from exported_tree_loading import load_data', 'from sentence_transformers import SentenceTransformer', 'from similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse', 'from sklearn.feature_extraction.text import CountVectorizer']\n",
            "functions: ['def argument_parsing():\\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\\n    parser.add_argument(\"--cores\", type=int, default=1)\\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\\n    parser.add_argument(\"--batch_size\", type=int, default=128)\\n    parser.add_argument(\"--k\", type=int, default=2)\\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\\n\\n    args = parser.parse_args()\\n    return args', 'def load_topic_model(args):\\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\\n    model = SentenceTransformer(MODEL_NAME)\\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\\n    topic_model = BERTopic(\\n        nr_topics=\"auto\",\\n        min_topic_size=args.min_topic_size,\\n        representation_model=representation_model,\\n        vectorizer_model=vectorizer_model,\\n        ctfidf_model=ctfidf_model,\\n        embedding_model=model,\\n    )\\n    return topic_model', 'def fit_topic_model(topic_model, data, embeddings, key=\"query\"):\\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\\n    return topics, probs', 'def get_topic_info(topic_model):\\n    return topic_model.get_topic_info()', 'def reduce_topics(topic_model, data, nr_topics, key=\"query\"):\\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\\n    return topic_model', 'def get_representative_docs(topic_model):\\n    return topic_model.get_representative_docs()', 'def reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\\n    if strategy == \"c-tf-idf\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\\n    elif strategy == \"embeddings\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\\n    elif strategy == \"distributions\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\\n    else:\\n        raise ValueError(\"Invalid strategy\")\\n    return new_topics', 'def compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\\n    tree = topic_model.get_topic_tree(hierarchical_topics)\\n    return hierarchical_topics, tree']\n",
            "Classes: []\n",
            "8 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/message_tree_topic_modeling.py\n",
            "Imports: ['import argparse', 'from bertopic import BERTopic', 'from bertopic.representation import MaximalMarginalRelevance', 'from bertopic.vectorizers import ClassTfidfTransformer', 'from exported_tree_loading import load_data', 'from sentence_transformers import SentenceTransformer', 'from similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse', 'from sklearn.feature_extraction.text import CountVectorizer']\n",
            "functions: ['def argument_parsing():\\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\\n    parser.add_argument(\"--cores\", type=int, default=1)\\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\\n    parser.add_argument(\"--batch_size\", type=int, default=128)\\n    parser.add_argument(\"--k\", type=int, default=2)\\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\\n\\n    args = parser.parse_args()\\n    return args', 'def load_topic_model(args):\\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\\n    model = SentenceTransformer(MODEL_NAME)\\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\\n    topic_model = BERTopic(\\n        nr_topics=\"auto\",\\n        min_topic_size=args.min_topic_size,\\n        representation_model=representation_model,\\n        vectorizer_model=vectorizer_model,\\n        ctfidf_model=ctfidf_model,\\n        embedding_model=model,\\n    )\\n    return topic_model', 'def fit_topic_model(topic_model, data, embeddings, key=\"query\"):\\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\\n    return topics, probs', 'def get_topic_info(topic_model):\\n    return topic_model.get_topic_info()', 'def reduce_topics(topic_model, data, nr_topics, key=\"query\"):\\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\\n    return topic_model', 'def get_representative_docs(topic_model):\\n    return topic_model.get_representative_docs()', 'def reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\\n    if strategy == \"c-tf-idf\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\\n    elif strategy == \"embeddings\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\\n    elif strategy == \"distributions\":\\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\\n    else:\\n        raise ValueError(\"Invalid strategy\")\\n    return new_topics', 'def compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\\n    tree = topic_model.get_topic_tree(hierarchical_topics)\\n    return hierarchical_topics, tree']\n",
            "Classes: []\n",
            "8 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/exported_tree_loading.py\n",
            "Imports: ['import json', 'from collections import defaultdict', 'from typing import List', 'import pandas as pd']\n",
            "functions: ['def load_jsonl(filepaths):\\n    data = []\\n    for filepath in filepaths:\\n        with open(filepath, \"r\") as f:\\n            for line in f:\\n                data.append(json.loads(line))\\n    return data', 'def separate_qa_helper(node, depth, msg_dict):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\\n        elif node[\"role\"] == \"assistant\":\\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                separate_qa_helper(reply, depth, msg_dict)', 'def store_qa_data_separate(trees, data):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def group_qa_helper(node, depth, msg_pairs):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            if \"replies\" in node:\\n                for reply in node[\"replies\"]:\\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\\n                    msg_pairs.append(qa_pair)\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                group_qa_helper(reply, depth, msg_pairs)', 'def store_qa_data_paired(trees, data: List):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def load_data(filepaths: List[str], paired=False):\\n    trees = load_jsonl(filepaths)\\n    if paired:\\n        data = []\\n        data, message_list = store_qa_data_paired(trees, data)\\n        sents = [f\"{qa[\\'instruct\\']} {qa[\\'answer\\']}\" for qa in data]\\n    elif not paired:\\n        data = defaultdict(list)\\n        data, message_list = store_qa_data_separate(trees, data)\\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\\n\\n    data = [(i, sent) for i, sent in enumerate(sents)]\\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\\n    return data, message_list']\n",
            "Classes: []\n",
            "6 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/exported_tree_loading.py\n",
            "Imports: ['import json', 'from collections import defaultdict', 'from typing import List', 'import pandas as pd']\n",
            "functions: ['def load_jsonl(filepaths):\\n    data = []\\n    for filepath in filepaths:\\n        with open(filepath, \"r\") as f:\\n            for line in f:\\n                data.append(json.loads(line))\\n    return data', 'def separate_qa_helper(node, depth, msg_dict):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\\n        elif node[\"role\"] == \"assistant\":\\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                separate_qa_helper(reply, depth, msg_dict)', 'def store_qa_data_separate(trees, data):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def group_qa_helper(node, depth, msg_pairs):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            if \"replies\" in node:\\n                for reply in node[\"replies\"]:\\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\\n                    msg_pairs.append(qa_pair)\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                group_qa_helper(reply, depth, msg_pairs)', 'def store_qa_data_paired(trees, data: List):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def load_data(filepaths: List[str], paired=False):\\n    trees = load_jsonl(filepaths)\\n    if paired:\\n        data = []\\n        data, message_list = store_qa_data_paired(trees, data)\\n        sents = [f\"{qa[\\'instruct\\']} {qa[\\'answer\\']}\" for qa in data]\\n    elif not paired:\\n        data = defaultdict(list)\\n        data, message_list = store_qa_data_separate(trees, data)\\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\\n\\n    data = [(i, sent) for i, sent in enumerate(sents)]\\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\\n    return data, message_list']\n",
            "Classes: []\n",
            "6 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/exported_tree_loading.py\n",
            "Imports: ['import json', 'from collections import defaultdict', 'from typing import List', 'import pandas as pd']\n",
            "functions: ['def load_jsonl(filepaths):\\n    data = []\\n    for filepath in filepaths:\\n        with open(filepath, \"r\") as f:\\n            for line in f:\\n                data.append(json.loads(line))\\n    return data', 'def separate_qa_helper(node, depth, msg_dict):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\\n        elif node[\"role\"] == \"assistant\":\\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                separate_qa_helper(reply, depth, msg_dict)', 'def store_qa_data_separate(trees, data):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def group_qa_helper(node, depth, msg_pairs):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            if \"replies\" in node:\\n                for reply in node[\"replies\"]:\\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\\n                    msg_pairs.append(qa_pair)\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                group_qa_helper(reply, depth, msg_pairs)', 'def store_qa_data_paired(trees, data: List):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def load_data(filepaths: List[str], paired=False):\\n    trees = load_jsonl(filepaths)\\n    if paired:\\n        data = []\\n        data, message_list = store_qa_data_paired(trees, data)\\n        sents = [f\"{qa[\\'instruct\\']} {qa[\\'answer\\']}\" for qa in data]\\n    elif not paired:\\n        data = defaultdict(list)\\n        data, message_list = store_qa_data_separate(trees, data)\\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\\n\\n    data = [(i, sent) for i, sent in enumerate(sents)]\\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\\n    return data, message_list']\n",
            "Classes: []\n",
            "6 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/exported_tree_loading.py\n",
            "Imports: ['import json', 'from collections import defaultdict', 'from typing import List', 'import pandas as pd']\n",
            "functions: ['def load_jsonl(filepaths):\\n    data = []\\n    for filepath in filepaths:\\n        with open(filepath, \"r\") as f:\\n            for line in f:\\n                data.append(json.loads(line))\\n    return data', 'def separate_qa_helper(node, depth, msg_dict):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\\n        elif node[\"role\"] == \"assistant\":\\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                separate_qa_helper(reply, depth, msg_dict)', 'def store_qa_data_separate(trees, data):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def group_qa_helper(node, depth, msg_pairs):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            if \"replies\" in node:\\n                for reply in node[\"replies\"]:\\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\\n                    msg_pairs.append(qa_pair)\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                group_qa_helper(reply, depth, msg_pairs)', 'def store_qa_data_paired(trees, data: List):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def load_data(filepaths: List[str], paired=False):\\n    trees = load_jsonl(filepaths)\\n    if paired:\\n        data = []\\n        data, message_list = store_qa_data_paired(trees, data)\\n        sents = [f\"{qa[\\'instruct\\']} {qa[\\'answer\\']}\" for qa in data]\\n    elif not paired:\\n        data = defaultdict(list)\\n        data, message_list = store_qa_data_separate(trees, data)\\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\\n\\n    data = [(i, sent) for i, sent in enumerate(sents)]\\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\\n    return data, message_list']\n",
            "Classes: []\n",
            "6 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/exported_tree_loading.py\n",
            "Imports: ['import json', 'from collections import defaultdict', 'from typing import List', 'import pandas as pd']\n",
            "functions: ['def load_jsonl(filepaths):\\n    data = []\\n    for filepath in filepaths:\\n        with open(filepath, \"r\") as f:\\n            for line in f:\\n                data.append(json.loads(line))\\n    return data', 'def separate_qa_helper(node, depth, msg_dict):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\\n        elif node[\"role\"] == \"assistant\":\\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                separate_qa_helper(reply, depth, msg_dict)', 'def store_qa_data_separate(trees, data):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def group_qa_helper(node, depth, msg_pairs):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            if \"replies\" in node:\\n                for reply in node[\"replies\"]:\\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\\n                    msg_pairs.append(qa_pair)\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                group_qa_helper(reply, depth, msg_pairs)', 'def store_qa_data_paired(trees, data: List):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def load_data(filepaths: List[str], paired=False):\\n    trees = load_jsonl(filepaths)\\n    if paired:\\n        data = []\\n        data, message_list = store_qa_data_paired(trees, data)\\n        sents = [f\"{qa[\\'instruct\\']} {qa[\\'answer\\']}\" for qa in data]\\n    elif not paired:\\n        data = defaultdict(list)\\n        data, message_list = store_qa_data_separate(trees, data)\\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\\n\\n    data = [(i, sent) for i, sent in enumerate(sents)]\\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\\n    return data, message_list']\n",
            "Classes: []\n",
            "6 0\n",
            "\n",
            "\n",
            "\n",
            "file_name: /content/Open-Assistant/backend/oasst_backend/utils/exported_tree_loading.py\n",
            "Imports: ['import json', 'from collections import defaultdict', 'from typing import List', 'import pandas as pd']\n",
            "functions: ['def load_jsonl(filepaths):\\n    data = []\\n    for filepath in filepaths:\\n        with open(filepath, \"r\") as f:\\n            for line in f:\\n                data.append(json.loads(line))\\n    return data', 'def separate_qa_helper(node, depth, msg_dict):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\\n        elif node[\"role\"] == \"assistant\":\\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                separate_qa_helper(reply, depth, msg_dict)', 'def store_qa_data_separate(trees, data):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def group_qa_helper(node, depth, msg_pairs):\\n    if \"text\" in node:\\n        if node[\"role\"] == \"prompter\":\\n            if \"replies\" in node:\\n                for reply in node[\"replies\"]:\\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\\n                    msg_pairs.append(qa_pair)\\n        depth += 1\\n        if \"replies\" in node:\\n            for reply in node[\"replies\"]:\\n                group_qa_helper(reply, depth, msg_pairs)', 'def store_qa_data_paired(trees, data: List):\\n    message_list = []\\n    for i, msg_tree in enumerate(trees):\\n        if \"prompt\" in msg_tree.keys():\\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\\n        elif \"prompt\" not in msg_tree.keys():\\n            message_list.append(msg_tree)\\n    return data, message_list', 'def load_data(filepaths: List[str], paired=False):\\n    trees = load_jsonl(filepaths)\\n    if paired:\\n        data = []\\n        data, message_list = store_qa_data_paired(trees, data)\\n        sents = [f\"{qa[\\'instruct\\']} {qa[\\'answer\\']}\" for qa in data]\\n    elif not paired:\\n        data = defaultdict(list)\\n        data, message_list = store_qa_data_separate(trees, data)\\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\\n\\n    data = [(i, sent) for i, sent in enumerate(sents)]\\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\\n    return data, message_list']\n",
            "Classes: []\n",
            "6 0\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modeling"
      ],
      "metadata": {
        "id": "7WPnklCCNoGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from collections import defaultdict\n",
        "from typing import List"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUvDlkrDN0mc",
        "outputId": "a9921e05-752f-4b15-b1f4-77c459c67c28"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jsonl(filepaths):\n",
        "    data = []\n",
        "    for filepath in filepaths:\n",
        "        with open(filepath, \"r\") as f:\n",
        "            for line in f:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "root_dir = \"/content/drive/MyDrive/open_assistant_utils_auto_mod\"\n",
        "repo = \"open_assistant\"\n",
        "repo_files = [f\"{root_dir}/{repo}_utils_summary.jsonl\", f'{root_dir}/{repo}_utils_todo_labeler.jsonl', f\"{root_dir}/{repo}_utils_bug_finder.jsonl\",\n",
        "         f\"{root_dir}/{repo}_utils_question_asking.jsonl\", f\"{root_dir}/{repo}_utils_complement_code.jsonl\"]\n",
        "\n",
        "res = load_jsonl(repo_files)\n",
        "sents = []\n",
        "for r in res:\n",
        "    reply =r['assistant_reply']\n",
        "    sents.append(reply)"
      ],
      "metadata": {
        "id": "-lUWw-GUQmX1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(sents, columns=[\"query\"])\n",
        "data[\"_id\"] = data.index"
      ],
      "metadata": {
        "id": "0WDkBPQOcF48"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_data(data, key='query', model_name='all-MiniLM-L6-v2', cores=1, gpu=False, batch_size=128):\n",
        "    \"\"\"\n",
        "    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n",
        "    \"\"\"\n",
        "    print('Embedding data')\n",
        "    model = SentenceTransformer(model_name)\n",
        "    print('Model loaded')\n",
        "\n",
        "    sentences = data[key].tolist()\n",
        "    unique_sentences = data[key].unique()\n",
        "    print('Unique sentences', len(unique_sentences))\n",
        "\n",
        "    if cores == 1:\n",
        "        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\n",
        "    else:\n",
        "        devices = ['cpu'] * cores\n",
        "        if gpu:\n",
        "            devices = None  # use all CUDA devices\n",
        "\n",
        "        # Start the multi-process pool on multiple devices\n",
        "        print('Multi-process pool starting')\n",
        "        pool = model.start_multi_process_pool(devices)\n",
        "        print('Multi-process pool started')\n",
        "\n",
        "        chunk_size = math.ceil(len(unique_sentences) / cores)\n",
        "\n",
        "        # Compute the embeddings using the multi-process pool\n",
        "        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\n",
        "        model.stop_multi_process_pool(pool)\n",
        "\n",
        "    print(\"Embeddings computed\")\n",
        "\n",
        "    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\n",
        "    embeddings = np.array([mapping[sentence] for sentence in sentences])\n",
        "  \n",
        "    return embeddings\n",
        "\n",
        "def gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\n",
        "    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\n",
        "    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\n",
        "    :param sigma: Width of the Gaussian kernel.\n",
        "    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\n",
        "    \"\"\"\n",
        "    if not isinstance(embs_a, torch.Tensor):\n",
        "        embs_a = torch.tensor(embs_a)\n",
        "\n",
        "    if not isinstance(embs_b, torch.Tensor):\n",
        "        embs_b = torch.tensor(embs_b)\n",
        "\n",
        "    # Compute the pairwise distances between the embeddings\n",
        "    dist_matrix = torch.cdist(embs_a, embs_b)\n",
        "\n",
        "    # Compute the Gaussian kernel matrix\n",
        "    kernel_matrix = torch.exp(-dist_matrix ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "    return kernel_matrix\n",
        "\n",
        "def cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(np.array(a))\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(np.array(b))\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "\n",
        "def prune_ref_docs(qa_embs, ref_embs, ref_docs, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Drops unnecessary documents from the reference embeddings and updates the list of reference documents,\n",
        "    and then recomputes the adjacency matrix.\n",
        "\n",
        "    Parameters:\n",
        "    qa_embs (numpy array): The embedding matrix of QA pairs.\n",
        "    ref_embs (numpy array): The embedding matrix of reference sentences.\n",
        "    ref_docs (list): The list of reference documents.\n",
        "    threshold (float): The threshold below which documents are considered unnecessary.\n",
        "\n",
        "    Returns:\n",
        "    pruned_ref_embs (numpy array): The pruned embedding matrix of reference sentences.\n",
        "    pruned_ref_docs (list): The pruned list of reference documents.\n",
        "    pruned_A (numpy array): The pruned adjacency matrix.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Compute the initial adjacency matrix with full reference embeddings\n",
        "    A = gaussian_kernel_torch(qa_embs, ref_embs, sigma=0.5)\n",
        "    print(f'Before: {A.shape}')\n",
        "    # Compute the row-wise sum of the adjacency matrix\n",
        "    row_sum = torch.sum(A, dim=0)\n",
        "    \n",
        "    # Identify the indexes of the relevant documents\n",
        "    relevant_idx = torch.where(row_sum > threshold * row_sum.max())[0]\n",
        "    \n",
        "    # Drop unnecessary rows from the reference embeddings\n",
        "    pruned_ref_embs = ref_embs[relevant_idx]\n",
        "    \n",
        "    # Update the list of reference documents\n",
        "    pruned_ref_docs = [ref_docs[i] for i in relevant_idx]\n",
        "    \n",
        "    # Recompute the adjacency matrix with pruned reference embeddings\n",
        "    pruned_A = gaussian_kernel_torch(qa_embs, pruned_ref_embs, sigma=0.5)\n",
        "    print(f'After: {pruned_A.shape}')\n",
        "    return pruned_ref_embs, pruned_ref_docs, pruned_A\n",
        "\n",
        "\n",
        "def compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\", sigma=1.0):\n",
        "    # match case to kernel type\n",
        "    if kernel_type == \"gaussian\":\n",
        "        A = gaussian_kernel_torch(embs, embs, sigma)\n",
        "    if kernel_type == \"cosine\":\n",
        "        A = cos_sim(embs, embs)\n",
        "    adj_matrix = torch.zeros_like(A)\n",
        "    adj_matrix[A > threshold] = 1\n",
        "    adj_matrix[A <= threshold] = 0\n",
        "    adj_matrix = adj_matrix.numpy().astype(np.float32)\n",
        "    return adj_matrix\n",
        "\n",
        "\n",
        "def k_hop_message_passing_sparse(A, node_features, k):\n",
        "    \"\"\"\n",
        "    Compute the k-hop adjacency matrix and aggregated features using message passing.\n",
        "\n",
        "    Parameters:\n",
        "    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\n",
        "    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\n",
        "    k (int): The number of hops for message passing.\n",
        "\n",
        "    Returns:\n",
        "    A_k (numpy array): The k-hop adjacency matrix.\n",
        "    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert input matrices to sparse matrices if they are not already\n",
        "    if not sp.issparse(A):\n",
        "        A = sp.csr_matrix(A)\n",
        "    if not sp.issparse(node_features):\n",
        "        node_features = sp.csr_matrix(node_features)\n",
        "\n",
        "    # Compute the k-hop adjacency matrix and the aggregated features\n",
        "    A_k = A.copy()\n",
        "    agg_features = node_features.copy()\n",
        "\n",
        "    for i in tqdm(range(k)):\n",
        "        # Compute the message passing for the k-hop neighborhood\n",
        "        message = A_k.dot(node_features)\n",
        "        # Apply a GCN layer to aggregate the messages\n",
        "        agg_features = A_k.dot(agg_features) + message\n",
        "        # Update the k-hop adjacency matrix by adding new edges\n",
        "        A_k += A_k.dot(A)\n",
        "\n",
        "    return A_k.toarray(), agg_features.toarray()\n",
        "\n",
        "def compute_kernel(embs, threshold=0.65, sigma=1.0):\n",
        "    # match case to kernel type\n",
        "    A = cos_sim(embs, embs, sigma)\n",
        "    adj_matrix = torch.zeros_like(A)\n",
        "    adj_matrix[A > threshold] = 1\n",
        "    adj_matrix[A <= threshold] = 0\n",
        "    adj_matrix = adj_matrix.numpy().astype(np.float32)\n",
        "    return adj_matrix"
      ],
      "metadata": {
        "id": "ubbE53YoN2iy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from bertopic.representation import OpenAI\n",
        "from bertopic.representation import MaximalMarginalRelevance\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "t6fHtasSegcB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_topic_model(diversity=0.1, min_topic_size=10):\n",
        "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "    representation_model = OpenAI(model=\"gpt-3.5-turbo\", delay_in_seconds=1, chat=True)\n",
        "    model = SentenceTransformer(MODEL_NAME)\n",
        "    #representation_model = MaximalMarginalRelevance(diversity=diversity)\n",
        "    topic_model = BERTopic(\n",
        "        nr_topics=\"auto\",\n",
        "        min_topic_size=min_topic_size,\n",
        "        representation_model=representation_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        ctfidf_model=ctfidf_model,\n",
        "        embedding_model=model,\n",
        "    )\n",
        "    return topic_model\n",
        "\n",
        "\n",
        "def fit_topic_model(topic_model, data, embeddings, key=\"query\"):\n",
        "    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\n",
        "    return topics, probs\n",
        "\n",
        "\n",
        "def get_topic_info(topic_model):\n",
        "    return topic_model.get_topic_info()\n",
        "\n",
        "\n",
        "def reduce_topics(topic_model, data, nr_topics, key=\"query\"):\n",
        "    topic_model.reduce_topics(data[key].to_list(), nr_topics)\n",
        "    return topic_model\n",
        "\n",
        "\n",
        "def get_representative_docs(topic_model):\n",
        "    return topic_model.get_representative_docs()\n",
        "\n",
        "\n",
        "def reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\n",
        "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "    representation_model = OpenAI(model=\"gpt-3.5-turbo\", delay_in_seconds=1, chat=True)\n",
        "    #representation_model = MaximalMarginalRelevance(diversity=diversity)\n",
        "    if strategy == \"c-tf-idf\":\n",
        "        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\n",
        "    elif strategy == \"embeddings\":\n",
        "        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\n",
        "    elif strategy == \"distributions\":\n",
        "        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid strategy\")\n",
        "    topic_model.update_topics(data[key].to_list(), topics=new_topics, representation_model=representation_model, vectorizer_model=vectorizer_model, ctfidf_model=ctfidf_model)\n",
        "    return topic_model, new_topics\n",
        "\n",
        "\n",
        "def compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\n",
        "    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\n",
        "    tree = topic_model.get_topic_tree(hierarchical_topics)\n",
        "    return hierarchical_topics, tree"
      ],
      "metadata": {
        "id": "IhGeZjyCemeD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "embs = embed_data(data)\n",
        "A = compute_cos_sim_kernel(embs, threshold=0.6)\n",
        "A_k, agg_features = k_hop_message_passing_sparse(A, embs, 2)\n",
        "topic_model = load_topic_model()\n",
        "topics, probs = fit_topic_model(topic_model, data, agg_features)\n",
        "freq = get_topic_info(topic_model)\n",
        "rep_docs = get_representative_docs(topic_model)\n",
        "freq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "ZkHM1vkgdvtI",
        "outputId": "e13227c5-dca8-4987-9224-e1a213220ca0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding data\n",
            "Model loaded\n",
            "Unique sentences 320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches: 100%|██████████| 3/3 [00:00<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings computed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 28.15it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Topic  Count                                               Name\n",
              "0      -1      1  -1_Defining the build_export_tree() Function t...\n",
              "1       0     58  0_Language classification pipeline in Open-Ass...\n",
              "2       1     51  1_Loading conversation data from JSONL files i...\n",
              "3       2     48  2_Bertopic model - reducing number of topics a...\n",
              "4       3     35  3_Recursive helper function for grouping messa...\n",
              "5       4     33  4_Suggestions for improving robustness of PyTo...\n",
              "6       5     33  5_Transaction Management with Asynchronous Fun...\n",
              "7       6     30  6_Absence of TODO comments in Open Assistant b...\n",
              "8       7     28  7_Methods for creating non-unique rankings usi...\n",
              "9       8     18  8_Writing JSON objects to file with compressio...\n",
              "10      9     11  9_k-hop message passing with sparse matrices a..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8e17997-a3db-4d0a-a554-0b56b1eb980d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1_Defining the build_export_tree() Function t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>0_Language classification pipeline in Open-Ass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>1_Loading conversation data from JSONL files i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>2_Bertopic model - reducing number of topics a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "      <td>3_Recursive helper function for grouping messa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>4_Suggestions for improving robustness of PyTo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>33</td>\n",
              "      <td>5_Transaction Management with Asynchronous Fun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>6_Absence of TODO comments in Open Assistant b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>7_Methods for creating non-unique rankings usi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>8_Writing JSON objects to file with compressio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>9_k-hop message passing with sparse matrices a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8e17997-a3db-4d0a-a554-0b56b1eb980d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8e17997-a3db-4d0a-a554-0b56b1eb980d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8e17997-a3db-4d0a-a554-0b56b1eb980d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hr, tree = compute_hierarchical_topic_tree(topic_model=topic_model, data=data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpwXk8cwfKAX",
        "outputId": "7f82f071-cb0a-4e3b-f2d7-57fed610f487"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:15<00:00,  1.69s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st-zad5bfLkC",
        "outputId": "3398f2b3-d711-4826-b1c2-4ab66ae9f90d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "├─■──Absence of TODO comments in Open Assistant backend ranking.py file____ ── Topic: 6\n",
            "└─Loading JSONL Data into Pandas DataFrame____\n",
            "     ├─Improving Functions for Computing Adjacency Matrices, Gaussian Kernels, and Rankings using PyTorch a\n",
            "     │    ├─Code structure and functionality improvements for matrix and kernel computation functions using PyTo\n",
            "     │    │    ├─■──Transaction Management with Asynchronous Functionality using Decorators____ ── Topic: 5\n",
            "     │    │    └─Python functions for matrix/tensor operations in graph-based machine learning____\n",
            "     │    │         ├─■──Suggestions for improving robustness of PyTorch similarity functions for computing kernel matrices b ── Topic: 4\n",
            "     │    │         └─■──k-hop message passing with sparse matrices and node features on a network graph____ ── Topic: 9\n",
            "     │    └─■──Methods for creating non-unique rankings using pairwise comparisons and tallies.____ ── Topic: 7\n",
            "     └─Topic: Functions for Loading and Storing Conversation Data in Python____\n",
            "          ├─Understanding the load_data function for loading conversation data into a pandas DataFrame from JSON\n",
            "          │    ├─■──Recursive helper function for grouping message pairs in a dialogue tree____ ── Topic: 3\n",
            "          │    └─■──Loading conversation data from JSONL files into a Pandas DataFrame____ ── Topic: 1\n",
            "          └─Language classification pipeline using a trained BERT model and pickle file for saving/loading____\n",
            "               ├─Improving language classification pipeline code and save_model function____\n",
            "               │    ├─■──Writing JSON objects to file with compression using Python____ ── Topic: 8\n",
            "               │    └─■──Language classification pipeline in Open-Assistant repository____ ── Topic: 0\n",
            "               └─■──Bertopic model - reducing number of topics and outliers____ ── Topic: 2\n",
            "\n"
          ]
        }
      ]
    }
  ]
}